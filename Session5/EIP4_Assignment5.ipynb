{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EIP4_Assignment5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuwedMszuVkl",
        "colab_type": "code",
        "outputId": "3b081d73-4c3c-4dcf-a091-a12b6553ce0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -q \"/content/gdrive/My Drive/hvc_info.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "replace resized/9733.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "adc.json            \u001b[0m\u001b[01;34mgdrive\u001b[0m/              \u001b[01;34mresized\u001b[0m/      \u001b[01;34msaved_models\u001b[0m/\n",
            "EIP4Assignment5.h5  hvc_annotations.csv  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HSabr2tvhvh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "import csv\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "from keras.layers.merge import concatenate\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, BatchNormalization, Activation\n",
        "from keras.layers import AveragePooling2D, Input, Flatten, Activation, Softmax\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOgAvJ8rwAEZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "annotations_csv = Path(\"hvc_annotations.csv\")\n",
        "images_root = Path(\"resized\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ee_t_Nj1xzkc",
        "colab_type": "code",
        "outputId": "b9be3071-62b8-4293-a935-f9ab49deedb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "ann_list = [ \n",
        "     dict({\n",
        "          \"file_name\": str(images_root/filename).split(\".jpg\")[0]+\".jpg\"},\n",
        "          **ann[\"regions\"][0][\"region_attributes\"]\n",
        "     ) for file_name, ann in tqdm(csv.DictReader(annotations_csv.read_text()))\n",
        "           \n",
        "]\n",
        "df = pd.DataFrame(ann_list[3:])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-8ab0877e9917>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m           \"file_name\": str(images_root/filename).split(\".jpg\")[0]+\".jpg\"},\n\u001b[1;32m      4\u001b[0m           \u001b[0;34m**\u001b[0m\u001b[0mann\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"regions\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"region_attributes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m      ) for file_name, ann in tqdm(csv.DictReader(annotations_csv.read_text()))\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m ]\n",
            "\u001b[0;32m<ipython-input-10-8ab0877e9917>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m           \"file_name\": str(images_root/filename).split(\".jpg\")[0]+\".jpg\"},\n\u001b[1;32m      4\u001b[0m           \u001b[0;34m**\u001b[0m\u001b[0mann\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"regions\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"region_attributes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m      ) for file_name, ann in tqdm(csv.DictReader(annotations_csv.read_text()))\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m ]\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azHEA0oAzsnR",
        "colab_type": "code",
        "outputId": "7a38bd79-e1a8-4963-ed1c-7e2c772f97f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "df[\"filename\"] # remove unwanted column\n",
        "df"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>gender</th>\n",
              "      <th>imagequality</th>\n",
              "      <th>age</th>\n",
              "      <th>weight</th>\n",
              "      <th>carryingbag</th>\n",
              "      <th>footwear</th>\n",
              "      <th>emotion</th>\n",
              "      <th>bodypose</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>images/Set1/5580_2 (3).jpg</td>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>images/Set1/4650_1 (4).jpg</td>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Angry/Serious</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>images/Set1/44880_0.jpg</td>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>images/Set1/26130_2.jpg</td>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>images/Set1/IMG (4438).jpg</td>\n",
              "      <td>female</td>\n",
              "      <td>Good</td>\n",
              "      <td>35-45</td>\n",
              "      <td>slightly-overweight</td>\n",
              "      <td>None</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13568</th>\n",
              "      <td>images/Set3/32880_0.jpg</td>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>slightly-overweight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Happy</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/13570.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13569</th>\n",
              "      <td>images/Set4/12210_0.jpg</td>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>25-35</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Fancy</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/13571.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13570</th>\n",
              "      <td>images/Set4/110190_3.jpg</td>\n",
              "      <td>female</td>\n",
              "      <td>Bad</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Side</td>\n",
              "      <td>resized/13572.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13571</th>\n",
              "      <td>images/Set4/4830_0.jpg</td>\n",
              "      <td>female</td>\n",
              "      <td>Bad</td>\n",
              "      <td>25-35</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/13573.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13572</th>\n",
              "      <td>images/Set4/5040_1.jpg</td>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>25-35</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Happy</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/13574.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13573 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                         filename  gender  ...        bodypose         image_path\n",
              "0      images/Set1/5580_2 (3).jpg    male  ...  Front-Frontish      resized/1.jpg\n",
              "1      images/Set1/4650_1 (4).jpg  female  ...  Front-Frontish      resized/2.jpg\n",
              "2         images/Set1/44880_0.jpg    male  ...  Front-Frontish      resized/3.jpg\n",
              "3         images/Set1/26130_2.jpg    male  ...  Front-Frontish      resized/4.jpg\n",
              "4      images/Set1/IMG (4438).jpg  female  ...  Front-Frontish      resized/5.jpg\n",
              "...                           ...     ...  ...             ...                ...\n",
              "13568     images/Set3/32880_0.jpg    male  ...  Front-Frontish  resized/13570.jpg\n",
              "13569     images/Set4/12210_0.jpg  female  ...  Front-Frontish  resized/13571.jpg\n",
              "13570    images/Set4/110190_3.jpg  female  ...            Side  resized/13572.jpg\n",
              "13571      images/Set4/4830_0.jpg  female  ...  Front-Frontish  resized/13573.jpg\n",
              "13572      images/Set4/5040_1.jpg    male  ...  Front-Frontish  resized/13574.jpg\n",
              "\n",
              "[13573 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH7Jw7e0zxss",
        "colab_type": "code",
        "outputId": "a980dff3-42d4-446f-f02f-5f42748e7ed7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        }
      },
      "source": [
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>image_path</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_female</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_male</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Average</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Good</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_15-25</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_25-35</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_35-45</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_45-55</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_55+</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_over-weight</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_underweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_None</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Normal</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Happy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Sad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Back</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Side</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  0  ...              4\n",
              "image_path                            resized/1.jpg  ...  resized/5.jpg\n",
              "gender_female                                     0  ...              1\n",
              "gender_male                                       1  ...              0\n",
              "imagequality_Average                              1  ...              0\n",
              "imagequality_Bad                                  0  ...              0\n",
              "imagequality_Good                                 0  ...              1\n",
              "age_15-25                                         0  ...              0\n",
              "age_25-35                                         0  ...              0\n",
              "age_35-45                                         1  ...              1\n",
              "age_45-55                                         0  ...              0\n",
              "age_55+                                           0  ...              0\n",
              "weight_normal-healthy                             1  ...              0\n",
              "weight_over-weight                                0  ...              0\n",
              "weight_slightly-overweight                        0  ...              1\n",
              "weight_underweight                                0  ...              0\n",
              "carryingbag_Daily/Office/Work Bag                 0  ...              0\n",
              "carryingbag_Grocery/Home/Plastic Bag              1  ...              0\n",
              "carryingbag_None                                  0  ...              1\n",
              "footwear_CantSee                                  0  ...              1\n",
              "footwear_Fancy                                    0  ...              0\n",
              "footwear_Normal                                   1  ...              0\n",
              "emotion_Angry/Serious                             0  ...              0\n",
              "emotion_Happy                                     0  ...              0\n",
              "emotion_Neutral                                   1  ...              1\n",
              "emotion_Sad                                       0  ...              0\n",
              "bodypose_Back                                     0  ...              0\n",
              "bodypose_Front-Frontish                           1  ...              1\n",
              "bodypose_Side                                     0  ...              0\n",
              "\n",
              "[28 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFnCXO8O1VYz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    \n",
        "    def __init__(self, df, batch_size=32, image_size=200, shuffle=True):\n",
        "        self.df = df\n",
        "        self.image_size=image_size - 24\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        image = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])\n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "        return image/255, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3-uPzL-1vaV",
        "colab_type": "code",
        "outputId": "5e0e79bc-50b7-40df-a6f5-a7ee260701f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.15, random_state=1)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11537, 28), (2036, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9NuniT41zX6",
        "colab_type": "code",
        "outputId": "a6134223-a011-4023-8e24-037ef3fc3427",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        }
      },
      "source": [
        "train_df"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>imagequality_Average</th>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <th>imagequality_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>carryingbag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>resized/59.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2106</th>\n",
              "      <td>resized/2107.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5206</th>\n",
              "      <td>resized/5207.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1163</th>\n",
              "      <td>resized/1164.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13534</th>\n",
              "      <td>resized/13536.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>905</th>\n",
              "      <td>resized/906.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5192</th>\n",
              "      <td>resized/5193.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12172</th>\n",
              "      <td>resized/12174.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>235</th>\n",
              "      <td>resized/236.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13349</th>\n",
              "      <td>resized/13351.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11537 rows × 28 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "58        resized/59.jpg              0  ...                        1              0\n",
              "2106    resized/2107.jpg              1  ...                        1              0\n",
              "5206    resized/5207.jpg              0  ...                        1              0\n",
              "1163    resized/1164.jpg              1  ...                        1              0\n",
              "13534  resized/13536.jpg              1  ...                        0              1\n",
              "...                  ...            ...  ...                      ...            ...\n",
              "905      resized/906.jpg              1  ...                        1              0\n",
              "5192    resized/5193.jpg              0  ...                        0              1\n",
              "12172  resized/12174.jpg              1  ...                        1              0\n",
              "235      resized/236.jpg              0  ...                        0              0\n",
              "13349  resized/13351.jpg              0  ...                        1              0\n",
              "\n",
              "[11537 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wqsa_lhDZd2b",
        "colab_type": "code",
        "outputId": "3e6b96e6-ef9b-455f-8d1a-f008b04f647d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        }
      },
      "source": [
        "val_df"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>imagequality_Average</th>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <th>imagequality_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>carryingbag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2450</th>\n",
              "      <td>resized/2451.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2743</th>\n",
              "      <td>resized/2744.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12207</th>\n",
              "      <td>resized/12209.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3931</th>\n",
              "      <td>resized/3932.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>994</th>\n",
              "      <td>resized/995.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5812</th>\n",
              "      <td>resized/5813.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7443</th>\n",
              "      <td>resized/7444.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8678</th>\n",
              "      <td>resized/8679.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9231</th>\n",
              "      <td>resized/9232.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8835</th>\n",
              "      <td>resized/8836.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2036 rows × 28 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "2450    resized/2451.jpg              1  ...                        0              0\n",
              "2743    resized/2744.jpg              1  ...                        0              1\n",
              "12207  resized/12209.jpg              1  ...                        1              0\n",
              "3931    resized/3932.jpg              1  ...                        1              0\n",
              "994      resized/995.jpg              1  ...                        1              0\n",
              "...                  ...            ...  ...                      ...            ...\n",
              "5812    resized/5813.jpg              1  ...                        1              0\n",
              "7443    resized/7444.jpg              0  ...                        0              1\n",
              "8678    resized/8679.jpg              1  ...                        0              0\n",
              "9231    resized/9232.jpg              1  ...                        1              0\n",
              "8835    resized/8836.jpg              0  ...                        1              0\n",
              "\n",
              "[2036 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iu5DZYlJ16oN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create train and validation data generators\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=32)\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=64, shuffle=False )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJJT3cL7zo9U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        if pixel_level:\n",
        "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "        input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyMyUOG59csD",
        "colab_type": "code",
        "outputId": "b593f886-fa8a-49c7-9454-b5f3c43ca811",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units\n",
        "val_images,_ = next(iter(valid_gen))\n",
        "cv2_imshow(val_images[0])\n",
        "print()\n",
        "print(f\"train data shape: {images[0].shape}, val data shape: {val_images[0].shape}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAAAuklEQVR4nO3WoQEAMAgDQbL/0pVY\nDKbciayQrwKAgaQXAAC2CE4AOEoEAMA34tkBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAID2ADHWAAlx\nAiXwAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=224x224 at 0x7F4B01A9A4A8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train data shape: (224, 224, 3), val data shape: (224, 224, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUuEu0hV32H8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_multi_catagories(x, n=3, order_dict=None):\n",
        "  \"\"\" x is numpy array\"\"\"\n",
        "  if order_dict is None:\n",
        "    x_encoded = LabelEncoder().fit_transform(x)\n",
        "    return np.eye(n)[x_encoded]\n",
        "\n",
        "  return np.eye(n)[order_dict[x]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sS7YTKgwaTtz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resize_and_pad(image, size=200, fill=0):\n",
        "    orig_size = image.shape[:2]\n",
        "    ratio = float(size)/max(orig_size)\n",
        "    new_size = tuple([int(x*ratio) for x in orig_size])\n",
        "    \n",
        "    image = cv2.resize(image, (new_size[1], new_size[0]))\n",
        "    \n",
        "    delta_w = size - new_size[1]\n",
        "    delta_h = size - new_size[0]\n",
        "    top, bottom = delta_h//2, delta_h - (delta_h//2)\n",
        "    left, right = delta_w//2, delta_w - (delta_w//2)\n",
        "    color = [fill]*3\n",
        "    return cv2.CopyMakeBorder(\n",
        "        image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXLxlmyp2UiO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_image(filename, resize=None, augment_fn=None):\n",
        "  image = cv2.imread(filename)\n",
        "  if resize is not None:\n",
        "     image = resize_and_pad(image, resize)\n",
        "  return image\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7P9uKyzXIuC",
        "colab_type": "code",
        "outputId": "ba27603d-721e-41dd-c325-3b670995ab39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "  x = read_image(df.file_name[10], resize=200)\n",
        "  df.iloc[10]\n",
        "  cv2_imshow(image)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-f30ce78d0fb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5177\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5178\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'file_name'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itGs6POZ8jX3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training parameters  \n",
        "epochs = 50\n",
        "data_augmentation = True\n",
        "num_classes = 8\n",
        "subtract_pixel_mean = True\n",
        "n=3\n",
        "version = 2\n",
        "\n",
        "if version == 1:\n",
        "    depth = n * 6 + 2\n",
        "elif version == 2:\n",
        "    depth = n * 9 + 2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWf9MCBi_6b4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_type = 'ResNet%dv%d' % (depth, version)\n",
        "input_shape = (224, 224, 3)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODRY9khxQa2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "\n",
        "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = 0.1\n",
        "    \n",
        "    if epoch < 50:\n",
        "        lr = 0.1\n",
        "    else:\n",
        "       lr = 0.2\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw7NtwK3DJbD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "\n",
        "    # Arguments\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "\n",
        "    # Returns\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGls6S-I9ITv",
        "colab_type": "code",
        "outputId": "bcc18ef8-8fd5-4eef-a397-e1547db488db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def resnet_v2(input_shape, depth, num_classes=8):\n",
        "    \"\"\"ResNet Version 2 Model builder [b]\n",
        "\n",
        "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
        "    bottleneck layer\n",
        "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
        "    Second and onwards shortcut connection is identity.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filter maps is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same filter map sizes.\n",
        "    Features maps sizes:\n",
        "    conv1  : 32x32,  16\n",
        "    stage 0: 32x32,  64\n",
        "    stage 1: 16x16, 128\n",
        "    stage 2:  8x8,  256\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 9 != 0:\n",
        "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
        "    # Start model definition.\n",
        "    num_filters_in = 16\n",
        "    num_res_blocks = int((depth - 2) / 9)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
        "    x = resnet_layer(inputs=inputs,\n",
        "                     num_filters=num_filters_in,\n",
        "                     conv_first=True)\n",
        "\n",
        "    # Instantiate the stack of residual units\n",
        "    for stage in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            activation = 'relu'\n",
        "            batch_normalization = True\n",
        "            strides = 1\n",
        "            if stage == 0:\n",
        "                num_filters_out = num_filters_in * 4\n",
        "                if res_block == 0:  # first layer and first stage\n",
        "                    activation = None\n",
        "                    batch_normalization = False\n",
        "            else:\n",
        "                num_filters_out = num_filters_in * 2\n",
        "                if res_block == 0:  # first layer but not first stage\n",
        "                    strides = 2    # downsample\n",
        "\n",
        "            # bottleneck residual unit\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters_in,\n",
        "                             kernel_size=1,\n",
        "                             strides=strides,\n",
        "                             activation=activation,\n",
        "                             batch_normalization=batch_normalization,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_in,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_out,\n",
        "                             kernel_size=1,\n",
        "                             conv_first=False)\n",
        "            if res_block == 0:\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters_out,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "\n",
        "        num_filters_in = num_filters_out\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v2 has BN-ReLU before Pooling\n",
        "    \n",
        "    #x = MaxPooling2D(name=\"MaxPooling\")(x)\n",
        "    x = Conv2D(8, 3, 3, name=\"last_layer\", border_mode=\"same\" )(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = AveragePooling2D(pool_size=56)(x)\n",
        "    y = Flatten()(x)\n",
        "    output = Softmax()(y)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=output)\n",
        "    return model\n",
        "    \n",
        "backbone = resnet_v2(input_shape, depth, num_classes)\n",
        "sc = backbone.output \n",
        "neck = backbone.output \n",
        "neck = Dense(512)(neck)\n",
        "\n",
        "def build_tower(in_layer):\n",
        "    neck = Dropout(0.1)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    neck = Dropout(0.2)(in_layer)\n",
        "    neck = Dense(128)(neck)\n",
        "    neck = concatenate([neck, sc])\n",
        "    return neck\n",
        "\n",
        "def build_head(name, in_layer):\n",
        "  return Dense(num_units[name], activation=\"softmax\", name=f\"{name}_output\")(in_layer)\n",
        "\n",
        "# heads\n",
        "gender = build_head(\"gender\", build_tower(neck))\n",
        "image_quality = build_head(\"image_quality\", build_tower(neck))\n",
        "age = build_head(\"age\", build_tower(neck))\n",
        "weight = build_head(\"weight\", build_tower(neck))\n",
        "bag = build_head(\"bag\", build_tower(neck))\n",
        "footwear = build_head(\"footwear\", build_tower(neck))\n",
        "emotion = build_head(\"emotion\", build_tower(neck))\n",
        "pose = build_head(\"pose\", build_tower(neck))\n",
        "\n",
        "   \n",
        "model = Model(inputs=backbone.input, outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion])\n",
        "\n",
        "if version == 2:\n",
        " model = Model(inputs=backbone.input, outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion]\n",
        ")\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=SGD(lr=lr_schedule(0), momentum=0.1),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "print(model_type)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:86: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), name=\"last_layer\", padding=\"same\")`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Learning rate:  0.1\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 224, 224, 16) 448         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 224, 224, 16) 64          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 224, 224, 16) 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 224, 224, 16) 272         activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 224, 224, 16) 64          conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 224, 224, 16) 0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 224, 224, 16) 2320        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 224, 224, 16) 64          conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 224, 224, 16) 0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 224, 224, 64) 1088        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 224, 224, 64) 1088        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 224, 224, 64) 0           conv2d_5[0][0]                   \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 224, 224, 64) 256         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 224, 224, 64) 0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 224, 224, 16) 1040        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 224, 224, 16) 64          conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 224, 224, 16) 0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 224, 224, 16) 2320        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 224, 224, 16) 64          conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 224, 224, 16) 0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 224, 224, 64) 1088        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 224, 224, 64) 0           add_1[0][0]                      \n",
            "                                                                 conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 224, 224, 64) 256         add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 224, 224, 64) 0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 224, 224, 16) 1040        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 224, 224, 16) 64          conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 224, 224, 16) 0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 224, 224, 16) 2320        activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 224, 224, 16) 64          conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 224, 224, 16) 0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 224, 224, 64) 1088        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 224, 224, 64) 0           add_2[0][0]                      \n",
            "                                                                 conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 224, 224, 64) 256         add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 224, 224, 64) 0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 112, 112, 64) 4160        activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 112, 112, 64) 256         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 112, 112, 64) 0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 112, 112, 64) 36928       activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 112, 112, 64) 256         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 112, 112, 64) 0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 112, 112, 128 8320        add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 112, 112, 128 8320        activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 112, 112, 128 0           conv2d_15[0][0]                  \n",
            "                                                                 conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 112, 112, 128 512         add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 112, 112, 128 0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 112, 112, 64) 8256        activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 112, 112, 64) 256         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 112, 112, 64) 0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 112, 112, 64) 36928       activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 112, 112, 64) 256         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 112, 112, 64) 0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 112, 112, 128 8320        activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 112, 112, 128 0           add_4[0][0]                      \n",
            "                                                                 conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 112, 112, 128 512         add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 112, 112, 128 0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 112, 112, 64) 8256        activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 112, 112, 64) 256         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 112, 112, 64) 0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 112, 112, 64) 36928       activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 112, 112, 64) 256         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 112, 112, 64) 0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 112, 112, 128 8320        activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 112, 112, 128 0           add_5[0][0]                      \n",
            "                                                                 conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 112, 112, 128 512         add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 112, 112, 128 0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 56, 56, 128)  16512       activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 56, 56, 128)  512         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 56, 56, 128)  0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 56, 56, 128)  147584      activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 56, 56, 128)  512         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 56, 56, 128)  0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 56, 56, 256)  33024       add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 56, 56, 256)  33024       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 56, 56, 256)  0           conv2d_25[0][0]                  \n",
            "                                                                 conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 56, 56, 256)  1024        add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 56, 56, 256)  0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 56, 56, 128)  32896       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 56, 56, 128)  512         conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 56, 56, 128)  0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 56, 56, 128)  147584      activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 56, 56, 128)  512         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 56, 56, 128)  0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 56, 56, 256)  33024       activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 56, 56, 256)  0           add_7[0][0]                      \n",
            "                                                                 conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 56, 56, 256)  1024        add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 56, 56, 256)  0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 56, 56, 128)  32896       activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 56, 56, 128)  512         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 56, 56, 128)  0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 56, 56, 128)  147584      activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 56, 56, 128)  512         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 56, 56, 128)  0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 56, 56, 256)  33024       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 56, 56, 256)  0           add_8[0][0]                      \n",
            "                                                                 conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "last_layer (Conv2D)             (None, 56, 56, 8)    18440       add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 56, 56, 8)    32          last_layer[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 1, 1, 8)      0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 8)            0           average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "softmax_1 (Softmax)             (None, 8)            0           flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 512)          4608        softmax_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_16 (Dropout)            (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 128)          65664       dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 128)          65664       dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 128)          65664       dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 128)          65664       dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 128)          65664       dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 128)          65664       dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_17 (Dense)                (None, 128)          65664       dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 128)          65664       dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dense_3[0][0]                    \n",
            "                                                                 softmax_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 136)          0           dense_5[0][0]                    \n",
            "                                                                 softmax_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 136)          0           dense_7[0][0]                    \n",
            "                                                                 softmax_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 136)          0           dense_9[0][0]                    \n",
            "                                                                 softmax_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 136)          0           dense_11[0][0]                   \n",
            "                                                                 softmax_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 136)          0           dense_13[0][0]                   \n",
            "                                                                 softmax_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 136)          0           dense_17[0][0]                   \n",
            "                                                                 softmax_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 136)          0           dense_15[0][0]                   \n",
            "                                                                 softmax_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "gender_output (Dense)           (None, 2)            274         concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "image_quality_output (Dense)    (None, 3)            411         concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "age_output (Dense)              (None, 5)            685         concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "weight_output (Dense)           (None, 4)            548         concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bag_output (Dense)              (None, 3)            411         concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "footwear_output (Dense)         (None, 3)            411         concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "pose_output (Dense)             (None, 3)            411         concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "emotion_output (Dense)          (None, 4)            548         concatenate_7[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 1,397,499\n",
            "Trainable params: 1,392,779\n",
            "Non-trainable params: 4,720\n",
            "__________________________________________________________________________________________________\n",
            "ResNet29v2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1teG0vwSTE7Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import Callback\n",
        "\n",
        "class LR_Finder(Callback):\n",
        "    \n",
        "    def __init__(self, start_lr=1e-5, end_lr=10, step_size=None, beta=.98):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.start_lr = start_lr\n",
        "        self.end_lr = end_lr\n",
        "        self.step_size = step_size\n",
        "        self.beta = beta\n",
        "        self.lr_mult = (end_lr/start_lr)**(1/step_size)\n",
        "        \n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.best_loss = 1e9\n",
        "        self.avg_loss = 0\n",
        "        self.losses, self.smoothed_losses, self.lrs, self.iterations = [], [], [], []\n",
        "        self.iteration = 0\n",
        "        logs = logs or {}\n",
        "        K.set_value(self.model.optimizer.lr, self.start_lr)\n",
        "        \n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        loss = logs.get('loss')\n",
        "        self.iteration += 1\n",
        "        \n",
        "        self.avg_loss = self.beta * self.avg_loss + (1 - self.beta) * loss\n",
        "        smoothed_loss = self.avg_loss / (1 - self.beta**self.iteration)\n",
        "        \n",
        "        # Check if the loss is not exploding\n",
        "        if self.iteration>1 and smoothed_loss > self.best_loss * 4:\n",
        "            self.model.stop_training = True\n",
        "            return\n",
        "\n",
        "        if smoothed_loss < self.best_loss or self.iteration==1:\n",
        "            self.best_loss = smoothed_loss\n",
        "        \n",
        "        lr = self.start_lr * (self.lr_mult**self.iteration)\n",
        "        \n",
        "        self.losses.append(loss)\n",
        "        self.smoothed_losses.append(smoothed_loss)\n",
        "        self.lrs.append(lr)\n",
        "        self.iterations.append(self.iteration)\n",
        "        \n",
        "        \n",
        "        K.set_value(self.model.optimizer.lr, lr)  \n",
        "        \n",
        "    def plot_lr(self):\n",
        "        plt.xlabel('Iterations')\n",
        "        plt.ylabel('Learning rate')\n",
        "        plt.plot(self.iterations, self.lrs)\n",
        "        \n",
        "    def plot(self, n_skip=10):\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel('Learning rate (log scale)')\n",
        "        plt.plot(self.lrs[n_skip:-5], self.losses[n_skip:-5])\n",
        "        plt.xscale('log')\n",
        "        \n",
        "    def plot_smoothed_loss(self, n_skip=10):\n",
        "        plt.ylabel('Smoothed Losses')\n",
        "        plt.xlabel('Learning rate (log scale)')\n",
        "        plt.plot(self.lrs[n_skip:-5], self.smoothed_losses[n_skip:-5])\n",
        "        plt.xscale('log')\n",
        "        \n",
        "    def plot_loss(self):\n",
        "        plt.ylabel('Losses')\n",
        "        plt.xlabel('Iterations')\n",
        "        plt.plot(self.iterations[10:], self.losses[10:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pThgOYWwTWVW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "7f86dd91-777d-4782-a1a0-0eb2a8011d65"
      },
      "source": [
        "import time\n",
        "\n",
        "batch_size=64\n",
        "# train the model\n",
        "start = time.time()\n",
        "# Train the model\n",
        "lr_finder = LR_Finder(start_lr=1e-5, end_lr=10, step_size=np.ceil(train_df.shape[0]/batch_size))\n",
        "model_info = model.fit_generator(generator=train_gen,\n",
        "                                 samples_per_epoch = train_df.shape[0], nb_epoch = 5, \n",
        "                                 validation_data = valid_gen, verbose=0,\n",
        "                                 callbacks=[lr_finder])\n",
        "end = time.time()\n",
        "print (\"Model took %0.2f seconds to train\"%(end - start))\n"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<__main__...., validation_data=<__main__...., verbose=0, callbacks=[<__main__..., steps_per_epoch=360, epochs=5)`\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model took 103.48 seconds to train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29FXEAKpWym7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "40b2201b-e10a-43d8-fd14-1d48dafadc35"
      },
      "source": [
        "lr_finder.plot_lr()"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEGCAYAAABvtY4XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXkQkRyngRURylAJeRCRHKeBFRHLU/wfp7nXe/vuoqAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4MJIV9hXPpB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "21a5d60a-f6eb-4f43-ea5e-a1d63ece5eda"
      },
      "source": [
        "lr_finder.plot()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3a9VAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWy87gweXR5t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "87ff8b34-fd20-4888-b254-ef9622e9ebd5"
      },
      "source": [
        "lr_finder.plot_smoothed_loss()"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nePX6anAR3okAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQDsNZHqXVZh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "a6810d63-cfdf-42fc-b830-c9ca1d4a7a51"
      },
      "source": [
        "lr_finder.plot_loss()"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nREQKlBRERKRASUFERAr+PzOYjgoXgLIQAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kREYLJXMR7ys",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4773cb00-31e9-447c-a98f-fa537c1c511b"
      },
      "source": [
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'EIP4Assignment5%s_model.{epoch:03d}.h5' % model_type\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "\n",
        "callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
        "\n",
        "# Run training, with or without data augmentation.\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    model.fit(train_gen,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=valid_gen,\n",
        "              shuffle=True,\n",
        "              callbacks=callbacks)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        # set input mean to 0 over the dataset\n",
        "        featurewise_center=False,\n",
        "        # set each sample mean to 0\n",
        "        samplewise_center=False,\n",
        "        # divide inputs by std of dataset\n",
        "        featurewise_std_normalization=False,\n",
        "        # divide each input by its std\n",
        "        samplewise_std_normalization=False,\n",
        "        # apply ZCA whitening\n",
        "        zca_whitening=False,\n",
        "        # epsilon for ZCA whitening\n",
        "        zca_epsilon=1e-06,\n",
        "        # randomly rotate images in the range (deg 0 to 180)\n",
        "        rotation_range=0,\n",
        "        # randomly shift images horizontally\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically\n",
        "        height_shift_range=0.1,\n",
        "        # set range for random shear\n",
        "        shear_range=0.,\n",
        "        # set range for random zoom\n",
        "        zoom_range=0.,\n",
        "        # set range for random channel shifts\n",
        "        channel_shift_range=0.,\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        # value used for fill_mode = \"constant\"\n",
        "        cval=0.,\n",
        "        # randomly flip images\n",
        "        horizontal_flip=True,\n",
        "        # randomly flip images\n",
        "        vertical_flip=False,\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using real-time data augmentation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19oSTiBCscDc",
        "colab_type": "code",
        "outputId": "15261c29-2513-4a7d-c7eb-8aba490dc336",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator # Imagedatagenerator library is used for data augmentation with no horizontal flips\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    preprocessing_function=get_random_eraser(v_l=0, v_h=1))\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "#lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               #cooldown=0,\n",
        "                               #patience=5,\n",
        "                               #min_lr=0.5e-6)\n",
        "\n",
        "callbacks = [checkpoint, lr_scheduler]\n",
        "\n",
        "    # Fit the model on the batches generated by datagen.flow().\n",
        "#model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                        #validation_data=(x_test, y_test),\n",
        "                        #epochs=epochs, verbose=1, workers=4,\n",
        "                        #callbacks=callbacks)\n",
        "\n",
        "\n",
        "\n",
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=50,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "Learning rate:  0.01\n",
            "360/360 [==============================] - 213s 591ms/step - loss: 8.5644 - gender_output_loss: 0.6853 - image_quality_output_loss: 0.9864 - age_output_loss: 1.4406 - weight_output_loss: 1.0129 - bag_output_loss: 0.9274 - footwear_output_loss: 1.0426 - pose_output_loss: 0.9358 - emotion_output_loss: 0.9231 - gender_output_acc: 0.5603 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4026 - weight_output_acc: 0.6331 - bag_output_acc: 0.5643 - footwear_output_acc: 0.4475 - pose_output_acc: 0.6156 - emotion_output_acc: 0.7142 - val_loss: 8.5774 - val_gender_output_loss: 0.6836 - val_image_quality_output_loss: 0.9802 - val_age_output_loss: 1.4466 - val_weight_output_loss: 0.9841 - val_bag_output_loss: 0.9374 - val_footwear_output_loss: 1.0424 - val_pose_output_loss: 0.9287 - val_emotion_output_loss: 0.9646 - val_gender_output_acc: 0.5640 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4461 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
            "  'skipping.' % (self.monitor), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "Learning rate:  0.01\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.4525 - gender_output_loss: 0.6847 - image_quality_output_loss: 0.9789 - age_output_loss: 1.4243 - weight_output_loss: 0.9841 - bag_output_loss: 0.9138 - footwear_output_loss: 1.0332 - pose_output_loss: 0.9266 - emotion_output_loss: 0.8974 - gender_output_acc: 0.5622 - image_quality_output_acc: 0.5532 - age_output_acc: 0.4045 - weight_output_acc: 0.6347 - bag_output_acc: 0.5667 - footwear_output_acc: 0.4647 - pose_output_acc: 0.6186 - emotion_output_acc: 0.7162Learning rate:  0.01\n",
            "360/360 [==============================] - 176s 488ms/step - loss: 8.4533 - gender_output_loss: 0.6847 - image_quality_output_loss: 0.9789 - age_output_loss: 1.4251 - weight_output_loss: 0.9840 - bag_output_loss: 0.9139 - footwear_output_loss: 1.0330 - pose_output_loss: 0.9269 - emotion_output_loss: 0.8974 - gender_output_acc: 0.5627 - image_quality_output_acc: 0.5533 - age_output_acc: 0.4042 - weight_output_acc: 0.6347 - bag_output_acc: 0.5667 - footwear_output_acc: 0.4649 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7161 - val_loss: 8.5639 - val_gender_output_loss: 0.6837 - val_image_quality_output_loss: 0.9794 - val_age_output_loss: 1.4457 - val_weight_output_loss: 0.9835 - val_bag_output_loss: 0.9372 - val_footwear_output_loss: 1.0350 - val_pose_output_loss: 0.9276 - val_emotion_output_loss: 0.9628 - val_gender_output_acc: 0.5645 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4773 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 3/50\n",
            "Learning rate:  0.01\n",
            "360/360 [==============================] - 176s 488ms/step - loss: 8.4404 - gender_output_loss: 0.6843 - image_quality_output_loss: 0.9768 - age_output_loss: 1.4252 - weight_output_loss: 0.9832 - bag_output_loss: 0.9134 - footwear_output_loss: 1.0250 - pose_output_loss: 0.9267 - emotion_output_loss: 0.8973 - gender_output_acc: 0.5622 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4040 - weight_output_acc: 0.6344 - bag_output_acc: 0.5666 - footwear_output_acc: 0.4944 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7162 - val_loss: 8.5513 - val_gender_output_loss: 0.6837 - val_image_quality_output_loss: 0.9772 - val_age_output_loss: 1.4450 - val_weight_output_loss: 0.9840 - val_bag_output_loss: 0.9366 - val_footwear_output_loss: 1.0241 - val_pose_output_loss: 0.9271 - val_emotion_output_loss: 0.9657 - val_gender_output_acc: 0.5645 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.5035 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 4/50\n",
            "Learning rate:  0.01\n",
            "360/360 [==============================] - 176s 488ms/step - loss: 8.4263 - gender_output_loss: 0.6847 - image_quality_output_loss: 0.9748 - age_output_loss: 1.4248 - weight_output_loss: 0.9825 - bag_output_loss: 0.9134 - footwear_output_loss: 1.0147 - pose_output_loss: 0.9267 - emotion_output_loss: 0.8974 - gender_output_acc: 0.5625 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4043 - weight_output_acc: 0.6347 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5204 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7162 - val_loss: 8.5361 - val_gender_output_loss: 0.6834 - val_image_quality_output_loss: 0.9751 - val_age_output_loss: 1.4456 - val_weight_output_loss: 0.9830 - val_bag_output_loss: 0.9380 - val_footwear_output_loss: 1.0108 - val_pose_output_loss: 0.9275 - val_emotion_output_loss: 0.9657 - val_gender_output_acc: 0.5645 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.5302 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 5/50\n",
            "Learning rate:  0.01\n",
            "360/360 [==============================] - 175s 487ms/step - loss: 8.4093 - gender_output_loss: 0.6833 - image_quality_output_loss: 0.9726 - age_output_loss: 1.4246 - weight_output_loss: 0.9825 - bag_output_loss: 0.9133 - footwear_output_loss: 1.0033 - pose_output_loss: 0.9265 - emotion_output_loss: 0.8966 - gender_output_acc: 0.5634 - image_quality_output_acc: 0.5532 - age_output_acc: 0.4040 - weight_output_acc: 0.6345 - bag_output_acc: 0.5665 - footwear_output_acc: 0.5337 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7161 - val_loss: 8.5219 - val_gender_output_loss: 0.6837 - val_image_quality_output_loss: 0.9720 - val_age_output_loss: 1.4464 - val_weight_output_loss: 0.9833 - val_bag_output_loss: 0.9359 - val_footwear_output_loss: 1.0010 - val_pose_output_loss: 0.9277 - val_emotion_output_loss: 0.9657 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.5323 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 6/50\n",
            "Learning rate:  0.01\n",
            "360/360 [==============================] - 175s 487ms/step - loss: 8.3901 - gender_output_loss: 0.6835 - image_quality_output_loss: 0.9687 - age_output_loss: 1.4244 - weight_output_loss: 0.9820 - bag_output_loss: 0.9127 - footwear_output_loss: 0.9905 - pose_output_loss: 0.9264 - emotion_output_loss: 0.8963 - gender_output_acc: 0.5626 - image_quality_output_acc: 0.5541 - age_output_acc: 0.4039 - weight_output_acc: 0.6347 - bag_output_acc: 0.5667 - footwear_output_acc: 0.5424 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7160 - val_loss: 8.5020 - val_gender_output_loss: 0.6832 - val_image_quality_output_loss: 0.9673 - val_age_output_loss: 1.4453 - val_weight_output_loss: 0.9825 - val_bag_output_loss: 0.9361 - val_footwear_output_loss: 0.9947 - val_pose_output_loss: 0.9268 - val_emotion_output_loss: 0.9609 - val_gender_output_acc: 0.5645 - val_image_quality_output_acc: 0.5549 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.5141 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 7/50\n",
            "Learning rate:  0.01\n",
            "360/360 [==============================] - 175s 486ms/step - loss: 8.3695 - gender_output_loss: 0.6832 - image_quality_output_loss: 0.9647 - age_output_loss: 1.4240 - weight_output_loss: 0.9811 - bag_output_loss: 0.9120 - footwear_output_loss: 0.9781 - pose_output_loss: 0.9259 - emotion_output_loss: 0.8960 - gender_output_acc: 0.5628 - image_quality_output_acc: 0.5548 - age_output_acc: 0.4043 - weight_output_acc: 0.6349 - bag_output_acc: 0.5665 - footwear_output_acc: 0.5494 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7161 - val_loss: 8.4918 - val_gender_output_loss: 0.6832 - val_image_quality_output_loss: 0.9648 - val_age_output_loss: 1.4436 - val_weight_output_loss: 0.9842 - val_bag_output_loss: 0.9361 - val_footwear_output_loss: 0.9800 - val_pose_output_loss: 0.9269 - val_emotion_output_loss: 0.9690 - val_gender_output_acc: 0.5640 - val_image_quality_output_acc: 0.5554 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.5433 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 8/50\n",
            "Learning rate:  0.01\n",
            "360/360 [==============================] - 175s 487ms/step - loss: 8.3537 - gender_output_loss: 0.6826 - image_quality_output_loss: 0.9595 - age_output_loss: 1.4238 - weight_output_loss: 0.9814 - bag_output_loss: 0.9116 - footwear_output_loss: 0.9692 - pose_output_loss: 0.9263 - emotion_output_loss: 0.8956 - gender_output_acc: 0.5632 - image_quality_output_acc: 0.5568 - age_output_acc: 0.4041 - weight_output_acc: 0.6345 - bag_output_acc: 0.5667 - footwear_output_acc: 0.5545 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7161 - val_loss: 8.4588 - val_gender_output_loss: 0.6811 - val_image_quality_output_loss: 0.9598 - val_age_output_loss: 1.4449 - val_weight_output_loss: 0.9830 - val_bag_output_loss: 0.9349 - val_footwear_output_loss: 0.9652 - val_pose_output_loss: 0.9270 - val_emotion_output_loss: 0.9597 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5529 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.5489 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 9/50\n",
            "Learning rate:  0.01\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.3373 - gender_output_loss: 0.6814 - image_quality_output_loss: 0.9564 - age_output_loss: 1.4237 - weight_output_loss: 0.9813 - bag_output_loss: 0.9110 - footwear_output_loss: 0.9610 - pose_output_loss: 0.9258 - emotion_output_loss: 0.8939 - gender_output_acc: 0.5618 - image_quality_output_acc: 0.5560 - age_output_acc: 0.4039 - weight_output_acc: 0.6345 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5576 - pose_output_acc: 0.6179 - emotion_output_acc: 0.7165Epoch 9/50\n",
            "360/360 [==============================] - 176s 488ms/step - loss: 8.3377 - gender_output_loss: 0.6813 - image_quality_output_loss: 0.9564 - age_output_loss: 1.4237 - weight_output_loss: 0.9811 - bag_output_loss: 0.9114 - footwear_output_loss: 0.9613 - pose_output_loss: 0.9259 - emotion_output_loss: 0.8940 - gender_output_acc: 0.5620 - image_quality_output_acc: 0.5563 - age_output_acc: 0.4043 - weight_output_acc: 0.6346 - bag_output_acc: 0.5665 - footwear_output_acc: 0.5571 - pose_output_acc: 0.6179 - emotion_output_acc: 0.7164 - val_loss: 8.4457 - val_gender_output_loss: 0.6812 - val_image_quality_output_loss: 0.9552 - val_age_output_loss: 1.4434 - val_weight_output_loss: 0.9825 - val_bag_output_loss: 0.9352 - val_footwear_output_loss: 0.9618 - val_pose_output_loss: 0.9267 - val_emotion_output_loss: 0.9575 - val_gender_output_acc: 0.5691 - val_image_quality_output_acc: 0.5524 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.5504 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 10/50\n",
            "Learning rate:  0.01\n",
            "360/360 [==============================] - 175s 487ms/step - loss: 8.3260 - gender_output_loss: 0.6802 - image_quality_output_loss: 0.9529 - age_output_loss: 1.4235 - weight_output_loss: 0.9810 - bag_output_loss: 0.9110 - footwear_output_loss: 0.9563 - pose_output_loss: 0.9251 - emotion_output_loss: 0.8943 - gender_output_acc: 0.5669 - image_quality_output_acc: 0.5564 - age_output_acc: 0.4036 - weight_output_acc: 0.6345 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5567 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7161 - val_loss: 8.4384 - val_gender_output_loss: 0.6790 - val_image_quality_output_loss: 0.9503 - val_age_output_loss: 1.4459 - val_weight_output_loss: 0.9850 - val_bag_output_loss: 0.9348 - val_footwear_output_loss: 0.9565 - val_pose_output_loss: 0.9270 - val_emotion_output_loss: 0.9587 - val_gender_output_acc: 0.5645 - val_image_quality_output_acc: 0.5519 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.5544 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 11/50\n",
            "Learning rate:  0.01\n",
            "360/360 [==============================] - 175s 487ms/step - loss: 8.3134 - gender_output_loss: 0.6793 - image_quality_output_loss: 0.9490 - age_output_loss: 1.4226 - weight_output_loss: 0.9803 - bag_output_loss: 0.9106 - footwear_output_loss: 0.9508 - pose_output_loss: 0.9257 - emotion_output_loss: 0.8943 - gender_output_acc: 0.5675 - image_quality_output_acc: 0.5578 - age_output_acc: 0.4042 - weight_output_acc: 0.6344 - bag_output_acc: 0.5664 - footwear_output_acc: 0.5624 - pose_output_acc: 0.6185 - emotion_output_acc: 0.7159 - val_loss: 8.4538 - val_gender_output_loss: 0.6782 - val_image_quality_output_loss: 0.9569 - val_age_output_loss: 1.4442 - val_weight_output_loss: 0.9834 - val_bag_output_loss: 0.9351 - val_footwear_output_loss: 0.9659 - val_pose_output_loss: 0.9267 - val_emotion_output_loss: 0.9631 - val_gender_output_acc: 0.5660 - val_image_quality_output_acc: 0.5519 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.5454 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 12/50\n",
            "Learning rate:  0.01\n",
            "360/360 [==============================] - 175s 487ms/step - loss: 8.3060 - gender_output_loss: 0.6787 - image_quality_output_loss: 0.9452 - age_output_loss: 1.4235 - weight_output_loss: 0.9809 - bag_output_loss: 0.9103 - footwear_output_loss: 0.9478 - pose_output_loss: 0.9257 - emotion_output_loss: 0.8942 - gender_output_acc: 0.5694 - image_quality_output_acc: 0.5574 - age_output_acc: 0.4036 - weight_output_acc: 0.6345 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5648 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7160 - val_loss: 8.4247 - val_gender_output_loss: 0.6786 - val_image_quality_output_loss: 0.9426 - val_age_output_loss: 1.4451 - val_weight_output_loss: 0.9837 - val_bag_output_loss: 0.9332 - val_footwear_output_loss: 0.9527 - val_pose_output_loss: 0.9272 - val_emotion_output_loss: 0.9623 - val_gender_output_acc: 0.5675 - val_image_quality_output_acc: 0.5529 - val_age_output_acc: 0.3664 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.5570 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 13/50\n",
            "Learning rate:  0.01\n",
            "360/360 [==============================] - 175s 487ms/step - loss: 8.2940 - gender_output_loss: 0.6778 - image_quality_output_loss: 0.9388 - age_output_loss: 1.4228 - weight_output_loss: 0.9805 - bag_output_loss: 0.9098 - footwear_output_loss: 0.9463 - pose_output_loss: 0.9255 - emotion_output_loss: 0.8937 - gender_output_acc: 0.5753 - image_quality_output_acc: 0.5582 - age_output_acc: 0.4039 - weight_output_acc: 0.6347 - bag_output_acc: 0.5671 - footwear_output_acc: 0.5639 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7161 - val_loss: 8.4453 - val_gender_output_loss: 0.6748 - val_image_quality_output_loss: 0.9542 - val_age_output_loss: 1.4432 - val_weight_output_loss: 0.9820 - val_bag_output_loss: 0.9360 - val_footwear_output_loss: 0.9693 - val_pose_output_loss: 0.9272 - val_emotion_output_loss: 0.9603 - val_gender_output_acc: 0.5771 - val_image_quality_output_acc: 0.5539 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.5307 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 14/50\n",
            "Learning rate:  0.01\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.2850 - gender_output_loss: 0.6774 - image_quality_output_loss: 0.9348 - age_output_loss: 1.4229 - weight_output_loss: 0.9798 - bag_output_loss: 0.9097 - footwear_output_loss: 0.9423 - pose_output_loss: 0.9265 - emotion_output_loss: 0.8937 - gender_output_acc: 0.5798 - image_quality_output_acc: 0.5619 - age_output_acc: 0.4039 - weight_output_acc: 0.6347 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5651 - pose_output_acc: 0.6178 - emotion_output_acc: 0.7160Epoch 14/50\n",
            "360/360 [==============================] - 175s 487ms/step - loss: 8.2835 - gender_output_loss: 0.6774 - image_quality_output_loss: 0.9348 - age_output_loss: 1.4227 - weight_output_loss: 0.9795 - bag_output_loss: 0.9097 - footwear_output_loss: 0.9424 - pose_output_loss: 0.9262 - emotion_output_loss: 0.8929 - gender_output_acc: 0.5796 - image_quality_output_acc: 0.5620 - age_output_acc: 0.4041 - weight_output_acc: 0.6349 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5648 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7163 - val_loss: 8.4305 - val_gender_output_loss: 0.6760 - val_image_quality_output_loss: 0.9493 - val_age_output_loss: 1.4447 - val_weight_output_loss: 0.9824 - val_bag_output_loss: 0.9377 - val_footwear_output_loss: 0.9562 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9602 - val_gender_output_acc: 0.5912 - val_image_quality_output_acc: 0.5514 - val_age_output_acc: 0.3664 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.5585 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 15/50\n",
            "Learning rate:  0.01\n",
            "360/360 [==============================] - 175s 487ms/step - loss: 8.2741 - gender_output_loss: 0.6764 - image_quality_output_loss: 0.9289 - age_output_loss: 1.4230 - weight_output_loss: 0.9804 - bag_output_loss: 0.9103 - footwear_output_loss: 0.9401 - pose_output_loss: 0.9253 - emotion_output_loss: 0.8927 - gender_output_acc: 0.5794 - image_quality_output_acc: 0.5614 - age_output_acc: 0.4036 - weight_output_acc: 0.6345 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5670 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7161 - val_loss: 8.3772 - val_gender_output_loss: 0.6729 - val_image_quality_output_loss: 0.9257 - val_age_output_loss: 1.4434 - val_weight_output_loss: 0.9819 - val_bag_output_loss: 0.9339 - val_footwear_output_loss: 0.9367 - val_pose_output_loss: 0.9267 - val_emotion_output_loss: 0.9595 - val_gender_output_acc: 0.5786 - val_image_quality_output_acc: 0.5494 - val_age_output_acc: 0.3664 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.5620 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 16/50\n",
            "Learning rate:  0.01\n",
            "360/360 [==============================] - 175s 487ms/step - loss: 8.2616 - gender_output_loss: 0.6745 - image_quality_output_loss: 0.9257 - age_output_loss: 1.4217 - weight_output_loss: 0.9803 - bag_output_loss: 0.9094 - footwear_output_loss: 0.9366 - pose_output_loss: 0.9253 - emotion_output_loss: 0.8921 - gender_output_acc: 0.5805 - image_quality_output_acc: 0.5603 - age_output_acc: 0.4031 - weight_output_acc: 0.6345 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5628 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7161 - val_loss: 8.5861 - val_gender_output_loss: 0.6780 - val_image_quality_output_loss: 0.9360 - val_age_output_loss: 1.4424 - val_weight_output_loss: 0.9862 - val_bag_output_loss: 0.9388 - val_footwear_output_loss: 1.1253 - val_pose_output_loss: 0.9254 - val_emotion_output_loss: 0.9584 - val_gender_output_acc: 0.5721 - val_image_quality_output_acc: 0.5514 - val_age_output_acc: 0.3664 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4178 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 17/50\n",
            "Learning rate:  0.01\n",
            "360/360 [==============================] - 175s 487ms/step - loss: 8.2513 - gender_output_loss: 0.6730 - image_quality_output_loss: 0.9204 - age_output_loss: 1.4214 - weight_output_loss: 0.9804 - bag_output_loss: 0.9096 - footwear_output_loss: 0.9339 - pose_output_loss: 0.9254 - emotion_output_loss: 0.8921 - gender_output_acc: 0.5852 - image_quality_output_acc: 0.5589 - age_output_acc: 0.4043 - weight_output_acc: 0.6347 - bag_output_acc: 0.5667 - footwear_output_acc: 0.5693 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7162 - val_loss: 8.9345 - val_gender_output_loss: 0.6780 - val_image_quality_output_loss: 0.9517 - val_age_output_loss: 1.4403 - val_weight_output_loss: 0.9895 - val_bag_output_loss: 0.9481 - val_footwear_output_loss: 1.4491 - val_pose_output_loss: 0.9267 - val_emotion_output_loss: 0.9566 - val_gender_output_acc: 0.5806 - val_image_quality_output_acc: 0.5539 - val_age_output_acc: 0.3674 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.3649 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 18/50\n",
            "Learning rate:  0.01\n",
            "360/360 [==============================] - 175s 486ms/step - loss: 8.2422 - gender_output_loss: 0.6722 - image_quality_output_loss: 0.9165 - age_output_loss: 1.4214 - weight_output_loss: 0.9800 - bag_output_loss: 0.9100 - footwear_output_loss: 0.9310 - pose_output_loss: 0.9257 - emotion_output_loss: 0.8913 - gender_output_acc: 0.5840 - image_quality_output_acc: 0.5611 - age_output_acc: 0.4041 - weight_output_acc: 0.6347 - bag_output_acc: 0.5667 - footwear_output_acc: 0.5727 - pose_output_acc: 0.6179 - emotion_output_acc: 0.7163 - val_loss: 8.3875 - val_gender_output_loss: 0.6789 - val_image_quality_output_loss: 0.9270 - val_age_output_loss: 1.4482 - val_weight_output_loss: 0.9835 - val_bag_output_loss: 0.9349 - val_footwear_output_loss: 0.9322 - val_pose_output_loss: 0.9259 - val_emotion_output_loss: 0.9633 - val_gender_output_acc: 0.5801 - val_image_quality_output_acc: 0.5565 - val_age_output_acc: 0.3669 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.5701 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 19/50\n",
            "Learning rate:  0.01\n",
            "360/360 [==============================] - 175s 487ms/step - loss: 8.2350 - gender_output_loss: 0.6712 - image_quality_output_loss: 0.9116 - age_output_loss: 1.4220 - weight_output_loss: 0.9798 - bag_output_loss: 0.9101 - footwear_output_loss: 0.9301 - pose_output_loss: 0.9250 - emotion_output_loss: 0.8921 - gender_output_acc: 0.5860 - image_quality_output_acc: 0.5601 - age_output_acc: 0.4033 - weight_output_acc: 0.6346 - bag_output_acc: 0.5665 - footwear_output_acc: 0.5729 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7160 - val_loss: 8.3814 - val_gender_output_loss: 0.6701 - val_image_quality_output_loss: 0.9134 - val_age_output_loss: 1.4403 - val_weight_output_loss: 0.9838 - val_bag_output_loss: 0.9347 - val_footwear_output_loss: 0.9592 - val_pose_output_loss: 0.9261 - val_emotion_output_loss: 0.9612 - val_gender_output_acc: 0.5948 - val_image_quality_output_acc: 0.5529 - val_age_output_acc: 0.3664 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.5474 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 20/50\n",
            "Learning rate:  0.01\n",
            "360/360 [==============================] - 175s 487ms/step - loss: 8.2247 - gender_output_loss: 0.6686 - image_quality_output_loss: 0.9079 - age_output_loss: 1.4211 - weight_output_loss: 0.9796 - bag_output_loss: 0.9100 - footwear_output_loss: 0.9290 - pose_output_loss: 0.9248 - emotion_output_loss: 0.8916 - gender_output_acc: 0.5915 - image_quality_output_acc: 0.5623 - age_output_acc: 0.4037 - weight_output_acc: 0.6347 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5733 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7163 - val_loss: 8.5083 - val_gender_output_loss: 0.6869 - val_image_quality_output_loss: 0.9715 - val_age_output_loss: 1.4511 - val_weight_output_loss: 0.9870 - val_bag_output_loss: 0.9337 - val_footwear_output_loss: 0.9879 - val_pose_output_loss: 0.9274 - val_emotion_output_loss: 0.9711 - val_gender_output_acc: 0.5549 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.3669 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5484 - val_footwear_output_acc: 0.5343 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 21/50\n",
            "Learning rate:  0.01\n",
            "360/360 [==============================] - 175s 487ms/step - loss: 8.2148 - gender_output_loss: 0.6680 - image_quality_output_loss: 0.9066 - age_output_loss: 1.4210 - weight_output_loss: 0.9804 - bag_output_loss: 0.9096 - footwear_output_loss: 0.9216 - pose_output_loss: 0.9247 - emotion_output_loss: 0.8916 - gender_output_acc: 0.5912 - image_quality_output_acc: 0.5617 - age_output_acc: 0.4039 - weight_output_acc: 0.6345 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5778 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7160 - val_loss: 8.3240 - val_gender_output_loss: 0.6628 - val_image_quality_output_loss: 0.9045 - val_age_output_loss: 1.4410 - val_weight_output_loss: 0.9818 - val_bag_output_loss: 0.9329 - val_footwear_output_loss: 0.9272 - val_pose_output_loss: 0.9255 - val_emotion_output_loss: 0.9575 - val_gender_output_acc: 0.6028 - val_image_quality_output_acc: 0.5529 - val_age_output_acc: 0.3669 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.5731 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 22/50\n",
            "Learning rate:  0.01\n",
            "360/360 [==============================] - 176s 488ms/step - loss: 8.2062 - gender_output_loss: 0.6649 - image_quality_output_loss: 0.9052 - age_output_loss: 1.4205 - weight_output_loss: 0.9799 - bag_output_loss: 0.9098 - footwear_output_loss: 0.9196 - pose_output_loss: 0.9250 - emotion_output_loss: 0.8912 - gender_output_acc: 0.5953 - image_quality_output_acc: 0.5609 - age_output_acc: 0.4037 - weight_output_acc: 0.6348 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5825 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7162 - val_loss: 9.0597 - val_gender_output_loss: 0.6646 - val_image_quality_output_loss: 0.9960 - val_age_output_loss: 1.4406 - val_weight_output_loss: 0.9922 - val_bag_output_loss: 0.9507 - val_footwear_output_loss: 1.5432 - val_pose_output_loss: 0.9273 - val_emotion_output_loss: 0.9553 - val_gender_output_acc: 0.5963 - val_image_quality_output_acc: 0.5222 - val_age_output_acc: 0.3695 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.3679 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 23/50\n",
            "Learning rate:  0.01\n",
            "360/360 [==============================] - 175s 487ms/step - loss: 8.1988 - gender_output_loss: 0.6629 - image_quality_output_loss: 0.9035 - age_output_loss: 1.4201 - weight_output_loss: 0.9802 - bag_output_loss: 0.9098 - footwear_output_loss: 0.9165 - pose_output_loss: 0.9246 - emotion_output_loss: 0.8919 - gender_output_acc: 0.6018 - image_quality_output_acc: 0.5590 - age_output_acc: 0.4038 - weight_output_acc: 0.6347 - bag_output_acc: 0.5665 - footwear_output_acc: 0.5820 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7158 - val_loss: 8.3804 - val_gender_output_loss: 0.6669 - val_image_quality_output_loss: 0.9240 - val_age_output_loss: 1.4468 - val_weight_output_loss: 0.9825 - val_bag_output_loss: 0.9337 - val_footwear_output_loss: 0.9487 - val_pose_output_loss: 0.9259 - val_emotion_output_loss: 0.9629 - val_gender_output_acc: 0.6028 - val_image_quality_output_acc: 0.5539 - val_age_output_acc: 0.3674 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.5539 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 24/50\n",
            "Learning rate:  0.01\n",
            "360/360 [==============================] - 175s 487ms/step - loss: 8.1852 - gender_output_loss: 0.6598 - image_quality_output_loss: 0.8995 - age_output_loss: 1.4197 - weight_output_loss: 0.9794 - bag_output_loss: 0.9093 - footwear_output_loss: 0.9132 - pose_output_loss: 0.9247 - emotion_output_loss: 0.8912 - gender_output_acc: 0.5960 - image_quality_output_acc: 0.5598 - age_output_acc: 0.4042 - weight_output_acc: 0.6349 - bag_output_acc: 0.5665 - footwear_output_acc: 0.5827 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7161 - val_loss: 8.3614 - val_gender_output_loss: 0.6637 - val_image_quality_output_loss: 0.9117 - val_age_output_loss: 1.4416 - val_weight_output_loss: 0.9848 - val_bag_output_loss: 0.9347 - val_footwear_output_loss: 0.9516 - val_pose_output_loss: 0.9275 - val_emotion_output_loss: 0.9579 - val_gender_output_acc: 0.5887 - val_image_quality_output_acc: 0.5509 - val_age_output_acc: 0.3674 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.5504 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 25/50\n",
            "Learning rate:  0.01\n",
            "360/360 [==============================] - 175s 487ms/step - loss: 8.1802 - gender_output_loss: 0.6560 - image_quality_output_loss: 0.9008 - age_output_loss: 1.4189 - weight_output_loss: 0.9793 - bag_output_loss: 0.9084 - footwear_output_loss: 0.9138 - pose_output_loss: 0.9243 - emotion_output_loss: 0.8911 - gender_output_acc: 0.6073 - image_quality_output_acc: 0.5634 - age_output_acc: 0.4043 - weight_output_acc: 0.6348 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5852 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7161 - val_loss: 8.4059 - val_gender_output_loss: 0.6517 - val_image_quality_output_loss: 0.9001 - val_age_output_loss: 1.4439 - val_weight_output_loss: 0.9833 - val_bag_output_loss: 0.9310 - val_footwear_output_loss: 1.0162 - val_pose_output_loss: 0.9275 - val_emotion_output_loss: 0.9653 - val_gender_output_acc: 0.6124 - val_image_quality_output_acc: 0.5524 - val_age_output_acc: 0.3669 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5469 - val_footwear_output_acc: 0.5212 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 26/50\n",
            "Learning rate:  0.01\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.1721 - gender_output_loss: 0.6534 - image_quality_output_loss: 0.8989 - age_output_loss: 1.4202 - weight_output_loss: 0.9807 - bag_output_loss: 0.9081 - footwear_output_loss: 0.9094 - pose_output_loss: 0.9241 - emotion_output_loss: 0.8907 - gender_output_acc: 0.6091 - image_quality_output_acc: 0.5642 - age_output_acc: 0.4035 - weight_output_acc: 0.6341 - bag_output_acc: 0.5667 - footwear_output_acc: 0.5884 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7161Epoch 26/50\n",
            "Learning rate:  0.01\n",
            "360/360 [==============================] - 176s 488ms/step - loss: 8.1705 - gender_output_loss: 0.6534 - image_quality_output_loss: 0.8987 - age_output_loss: 1.4198 - weight_output_loss: 0.9802 - bag_output_loss: 0.9081 - footwear_output_loss: 0.9092 - pose_output_loss: 0.9242 - emotion_output_loss: 0.8903 - gender_output_acc: 0.6095 - image_quality_output_acc: 0.5644 - age_output_acc: 0.4037 - weight_output_acc: 0.6345 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5885 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7162 - val_loss: 8.4973 - val_gender_output_loss: 0.7677 - val_image_quality_output_loss: 0.9268 - val_age_output_loss: 1.4424 - val_weight_output_loss: 0.9811 - val_bag_output_loss: 0.9339 - val_footwear_output_loss: 0.9743 - val_pose_output_loss: 0.9260 - val_emotion_output_loss: 0.9588 - val_gender_output_acc: 0.4446 - val_image_quality_output_acc: 0.5580 - val_age_output_acc: 0.3730 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5469 - val_footwear_output_acc: 0.5262 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 27/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 175s 487ms/step - loss: 8.2677 - gender_output_loss: 0.6675 - image_quality_output_loss: 0.9154 - age_output_loss: 1.4259 - weight_output_loss: 0.9847 - bag_output_loss: 0.9144 - footwear_output_loss: 0.9550 - pose_output_loss: 0.9275 - emotion_output_loss: 0.8955 - gender_output_acc: 0.5903 - image_quality_output_acc: 0.5539 - age_output_acc: 0.4032 - weight_output_acc: 0.6345 - bag_output_acc: 0.5658 - footwear_output_acc: 0.5547 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7163 - val_loss: 8.3690 - val_gender_output_loss: 0.6533 - val_image_quality_output_loss: 0.9229 - val_age_output_loss: 1.4452 - val_weight_output_loss: 0.9834 - val_bag_output_loss: 0.9337 - val_footwear_output_loss: 0.9492 - val_pose_output_loss: 0.9353 - val_emotion_output_loss: 0.9686 - val_gender_output_acc: 0.6124 - val_image_quality_output_acc: 0.5554 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.5585 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 28/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 175s 487ms/step - loss: 8.2034 - gender_output_loss: 0.6544 - image_quality_output_loss: 0.9043 - age_output_loss: 1.4221 - weight_output_loss: 0.9825 - bag_output_loss: 0.9115 - footwear_output_loss: 0.9357 - pose_output_loss: 0.9265 - emotion_output_loss: 0.8936 - gender_output_acc: 0.6141 - image_quality_output_acc: 0.5591 - age_output_acc: 0.4054 - weight_output_acc: 0.6345 - bag_output_acc: 0.5656 - footwear_output_acc: 0.5694 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7160 - val_loss: 9.0683 - val_gender_output_loss: 0.7954 - val_image_quality_output_loss: 0.9392 - val_age_output_loss: 1.4330 - val_weight_output_loss: 0.9958 - val_bag_output_loss: 0.9327 - val_footwear_output_loss: 1.5036 - val_pose_output_loss: 0.9443 - val_emotion_output_loss: 0.9556 - val_gender_output_acc: 0.4516 - val_image_quality_output_acc: 0.5489 - val_age_output_acc: 0.3674 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5464 - val_footwear_output_acc: 0.3659 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 29/50\n",
            "Learning rate:  0.1\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.1482 - gender_output_loss: 0.6409 - image_quality_output_loss: 0.8989 - age_output_loss: 1.4192 - weight_output_loss: 0.9820 - bag_output_loss: 0.9075 - footwear_output_loss: 0.9200 - pose_output_loss: 0.9238 - emotion_output_loss: 0.8916 - gender_output_acc: 0.6260 - image_quality_output_acc: 0.5635 - age_output_acc: 0.4032 - weight_output_acc: 0.6346 - bag_output_acc: 0.5664 - footwear_output_acc: 0.5756 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7162\n",
            "360/360 [==============================] - 175s 487ms/step - loss: 8.1482 - gender_output_loss: 0.6409 - image_quality_output_loss: 0.8990 - age_output_loss: 1.4191 - weight_output_loss: 0.9820 - bag_output_loss: 0.9072 - footwear_output_loss: 0.9202 - pose_output_loss: 0.9239 - emotion_output_loss: 0.8916 - gender_output_acc: 0.6260 - image_quality_output_acc: 0.5632 - age_output_acc: 0.4031 - weight_output_acc: 0.6347 - bag_output_acc: 0.5667 - footwear_output_acc: 0.5753 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7161 - val_loss: 9.2379 - val_gender_output_loss: 0.9144 - val_image_quality_output_loss: 0.9424 - val_age_output_loss: 1.4437 - val_weight_output_loss: 0.9911 - val_bag_output_loss: 0.9334 - val_footwear_output_loss: 1.5293 - val_pose_output_loss: 0.9490 - val_emotion_output_loss: 0.9743 - val_gender_output_acc: 0.4602 - val_image_quality_output_acc: 0.5035 - val_age_output_acc: 0.3679 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.3730 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 30/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 175s 486ms/step - loss: 8.0999 - gender_output_loss: 0.6290 - image_quality_output_loss: 0.8949 - age_output_loss: 1.4163 - weight_output_loss: 0.9800 - bag_output_loss: 0.9011 - footwear_output_loss: 0.9100 - pose_output_loss: 0.9231 - emotion_output_loss: 0.8894 - gender_output_acc: 0.6460 - image_quality_output_acc: 0.5645 - age_output_acc: 0.4033 - weight_output_acc: 0.6345 - bag_output_acc: 0.5686 - footwear_output_acc: 0.5822 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7163 - val_loss: 10.9369 - val_gender_output_loss: 0.8556 - val_image_quality_output_loss: 1.0976 - val_age_output_loss: 1.4376 - val_weight_output_loss: 1.0287 - val_bag_output_loss: 0.9374 - val_footwear_output_loss: 3.0809 - val_pose_output_loss: 0.9624 - val_emotion_output_loss: 0.9849 - val_gender_output_acc: 0.4375 - val_image_quality_output_acc: 0.3090 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5383 - val_footwear_output_acc: 0.3649 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 31/50\n",
            "Learning rate:  0.1\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.0603 - gender_output_loss: 0.6227 - image_quality_output_loss: 0.8918 - age_output_loss: 1.4163 - weight_output_loss: 0.9796 - bag_output_loss: 0.8983 - footwear_output_loss: 0.8929 - pose_output_loss: 0.9220 - emotion_output_loss: 0.8889 - gender_output_acc: 0.6449 - image_quality_output_acc: 0.5693 - age_output_acc: 0.4046 - weight_output_acc: 0.6347 - bag_output_acc: 0.5704 - footwear_output_acc: 0.5945 - pose_output_acc: 0.6179 - emotion_output_acc: 0.7161\n",
            "360/360 [==============================] - 176s 488ms/step - loss: 8.0597 - gender_output_loss: 0.6227 - image_quality_output_loss: 0.8919 - age_output_loss: 1.4160 - weight_output_loss: 0.9801 - bag_output_loss: 0.8981 - footwear_output_loss: 0.8929 - pose_output_loss: 0.9216 - emotion_output_loss: 0.8885 - gender_output_acc: 0.6451 - image_quality_output_acc: 0.5693 - age_output_acc: 0.4046 - weight_output_acc: 0.6346 - bag_output_acc: 0.5702 - footwear_output_acc: 0.5944 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7162 - val_loss: 8.9828 - val_gender_output_loss: 0.9689 - val_image_quality_output_loss: 0.9115 - val_age_output_loss: 1.4999 - val_weight_output_loss: 1.0153 - val_bag_output_loss: 1.0351 - val_footwear_output_loss: 1.0694 - val_pose_output_loss: 0.9598 - val_emotion_output_loss: 0.9790 - val_gender_output_acc: 0.5696 - val_image_quality_output_acc: 0.5640 - val_age_output_acc: 0.3574 - val_weight_output_acc: 0.6381 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.5010 - val_pose_output_acc: 0.6124 - val_emotion_output_acc: 0.6845\n",
            "Epoch 32/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 488ms/step - loss: 8.0244 - gender_output_loss: 0.6170 - image_quality_output_loss: 0.8907 - age_output_loss: 1.4113 - weight_output_loss: 0.9784 - bag_output_loss: 0.8943 - footwear_output_loss: 0.8850 - pose_output_loss: 0.9216 - emotion_output_loss: 0.8863 - gender_output_acc: 0.6578 - image_quality_output_acc: 0.5692 - age_output_acc: 0.4046 - weight_output_acc: 0.6345 - bag_output_acc: 0.5739 - footwear_output_acc: 0.6003 - pose_output_acc: 0.6179 - emotion_output_acc: 0.7162 - val_loss: 8.5982 - val_gender_output_loss: 0.6742 - val_image_quality_output_loss: 0.9863 - val_age_output_loss: 1.4640 - val_weight_output_loss: 0.9857 - val_bag_output_loss: 0.9338 - val_footwear_output_loss: 1.1345 - val_pose_output_loss: 0.9255 - val_emotion_output_loss: 0.9582 - val_gender_output_acc: 0.6144 - val_image_quality_output_acc: 0.5066 - val_age_output_acc: 0.3684 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5439 - val_footwear_output_acc: 0.4889 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 33/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 488ms/step - loss: 7.9926 - gender_output_loss: 0.6099 - image_quality_output_loss: 0.8851 - age_output_loss: 1.4118 - weight_output_loss: 0.9784 - bag_output_loss: 0.8914 - footwear_output_loss: 0.8756 - pose_output_loss: 0.9203 - emotion_output_loss: 0.8881 - gender_output_acc: 0.6602 - image_quality_output_acc: 0.5717 - age_output_acc: 0.4043 - weight_output_acc: 0.6347 - bag_output_acc: 0.5697 - footwear_output_acc: 0.6047 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7158 - val_loss: 8.2866 - val_gender_output_loss: 0.6420 - val_image_quality_output_loss: 0.9320 - val_age_output_loss: 1.4413 - val_weight_output_loss: 0.9878 - val_bag_output_loss: 0.9373 - val_footwear_output_loss: 0.9201 - val_pose_output_loss: 0.9455 - val_emotion_output_loss: 0.9525 - val_gender_output_acc: 0.6184 - val_image_quality_output_acc: 0.5323 - val_age_output_acc: 0.3669 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5484 - val_footwear_output_acc: 0.5655 - val_pose_output_acc: 0.6134 - val_emotion_output_acc: 0.6845\n",
            "Epoch 34/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 488ms/step - loss: 7.9535 - gender_output_loss: 0.6022 - image_quality_output_loss: 0.8832 - age_output_loss: 1.4095 - weight_output_loss: 0.9770 - bag_output_loss: 0.8888 - footwear_output_loss: 0.8639 - pose_output_loss: 0.9195 - emotion_output_loss: 0.8850 - gender_output_acc: 0.6651 - image_quality_output_acc: 0.5753 - age_output_acc: 0.4023 - weight_output_acc: 0.6347 - bag_output_acc: 0.5753 - footwear_output_acc: 0.6075 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7165 - val_loss: 8.3129 - val_gender_output_loss: 0.7235 - val_image_quality_output_loss: 0.8883 - val_age_output_loss: 1.4498 - val_weight_output_loss: 0.9930 - val_bag_output_loss: 0.9379 - val_footwear_output_loss: 0.9090 - val_pose_output_loss: 0.9289 - val_emotion_output_loss: 0.9619 - val_gender_output_acc: 0.5408 - val_image_quality_output_acc: 0.5635 - val_age_output_acc: 0.3664 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5166 - val_footwear_output_acc: 0.5811 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 35/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 488ms/step - loss: 7.9230 - gender_output_loss: 0.5916 - image_quality_output_loss: 0.8777 - age_output_loss: 1.4100 - weight_output_loss: 0.9767 - bag_output_loss: 0.8853 - footwear_output_loss: 0.8625 - pose_output_loss: 0.9178 - emotion_output_loss: 0.8844 - gender_output_acc: 0.6760 - image_quality_output_acc: 0.5789 - age_output_acc: 0.4033 - weight_output_acc: 0.6346 - bag_output_acc: 0.5802 - footwear_output_acc: 0.6131 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7161 - val_loss: 9.4000 - val_gender_output_loss: 0.7853 - val_image_quality_output_loss: 0.9271 - val_age_output_loss: 1.4629 - val_weight_output_loss: 1.0167 - val_bag_output_loss: 1.0579 - val_footwear_output_loss: 1.7191 - val_pose_output_loss: 0.9503 - val_emotion_output_loss: 0.9674 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5630 - val_age_output_acc: 0.3584 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5484 - val_footwear_output_acc: 0.3705 - val_pose_output_acc: 0.6154 - val_emotion_output_acc: 0.6845\n",
            "Epoch 36/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 175s 487ms/step - loss: 7.8978 - gender_output_loss: 0.5827 - image_quality_output_loss: 0.8804 - age_output_loss: 1.4080 - weight_output_loss: 0.9768 - bag_output_loss: 0.8820 - footwear_output_loss: 0.8560 - pose_output_loss: 0.9173 - emotion_output_loss: 0.8849 - gender_output_acc: 0.6901 - image_quality_output_acc: 0.5803 - age_output_acc: 0.4032 - weight_output_acc: 0.6345 - bag_output_acc: 0.5822 - footwear_output_acc: 0.6116 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7160 - val_loss: 11.5699 - val_gender_output_loss: 0.6203 - val_image_quality_output_loss: 1.0639 - val_age_output_loss: 1.4721 - val_weight_output_loss: 1.0247 - val_bag_output_loss: 0.9584 - val_footwear_output_loss: 3.9846 - val_pose_output_loss: 0.9548 - val_emotion_output_loss: 0.9850 - val_gender_output_acc: 0.6618 - val_image_quality_output_acc: 0.3831 - val_age_output_acc: 0.2787 - val_weight_output_acc: 0.6391 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.3649 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 37/50\n",
            "Learning rate:  0.1\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.8669 - gender_output_loss: 0.5715 - image_quality_output_loss: 0.8770 - age_output_loss: 1.4078 - weight_output_loss: 0.9763 - bag_output_loss: 0.8797 - footwear_output_loss: 0.8535 - pose_output_loss: 0.9147 - emotion_output_loss: 0.8838 - gender_output_acc: 0.6938 - image_quality_output_acc: 0.5815 - age_output_acc: 0.4025 - weight_output_acc: 0.6347 - bag_output_acc: 0.5818 - footwear_output_acc: 0.6146 - pose_output_acc: 0.6179 - emotion_output_acc: 0.7158Epoch 37/50\n",
            "360/360 [==============================] - 176s 489ms/step - loss: 7.8674 - gender_output_loss: 0.5716 - image_quality_output_loss: 0.8770 - age_output_loss: 1.4080 - weight_output_loss: 0.9767 - bag_output_loss: 0.8796 - footwear_output_loss: 0.8537 - pose_output_loss: 0.9147 - emotion_output_loss: 0.8834 - gender_output_acc: 0.6935 - image_quality_output_acc: 0.5815 - age_output_acc: 0.4025 - weight_output_acc: 0.6345 - bag_output_acc: 0.5815 - footwear_output_acc: 0.6143 - pose_output_acc: 0.6179 - emotion_output_acc: 0.7160 - val_loss: 8.3528 - val_gender_output_loss: 0.6632 - val_image_quality_output_loss: 1.0386 - val_age_output_loss: 1.4411 - val_weight_output_loss: 0.9813 - val_bag_output_loss: 0.9180 - val_footwear_output_loss: 0.9009 - val_pose_output_loss: 0.9479 - val_emotion_output_loss: 0.9625 - val_gender_output_acc: 0.5948 - val_image_quality_output_acc: 0.4803 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5635 - val_footwear_output_acc: 0.5822 - val_pose_output_acc: 0.6154 - val_emotion_output_acc: 0.6845\n",
            "Epoch 38/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 489ms/step - loss: 7.8674 - gender_output_loss: 0.5716 - image_quality_output_loss: 0.8770 - age_output_loss: 1.4080 - weight_output_loss: 0.9767 - bag_output_loss: 0.8796 - footwear_output_loss: 0.8537 - pose_output_loss: 0.9147 - emotion_output_loss: 0.8834 - gender_output_acc: 0.6935 - image_quality_output_acc: 0.5815 - age_output_acc: 0.4025 - weight_output_acc: 0.6345 - bag_output_acc: 0.5815 - footwear_output_acc: 0.6143 - pose_output_acc: 0.6179 - emotion_output_acc: 0.7160 - val_loss: 8.3528 - val_gender_output_loss: 0.6632 - val_image_quality_output_loss: 1.0386 - val_age_output_loss: 1.4411 - val_weight_output_loss: 0.9813 - val_bag_output_loss: 0.9180 - val_footwear_output_loss: 0.9009 - val_pose_output_loss: 0.9479 - val_emotion_output_loss: 0.9625 - val_gender_output_acc: 0.5948 - val_image_quality_output_acc: 0.4803 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5635 - val_footwear_output_acc: 0.5822 - val_pose_output_acc: 0.6154 - val_emotion_output_acc: 0.6845\n",
            "360/360 [==============================] - 176s 489ms/step - loss: 7.8237 - gender_output_loss: 0.5611 - image_quality_output_loss: 0.8739 - age_output_loss: 1.4048 - weight_output_loss: 0.9747 - bag_output_loss: 0.8722 - footwear_output_loss: 0.8427 - pose_output_loss: 0.9166 - emotion_output_loss: 0.8819 - gender_output_acc: 0.7059 - image_quality_output_acc: 0.5852 - age_output_acc: 0.4037 - weight_output_acc: 0.6346 - bag_output_acc: 0.5947 - footwear_output_acc: 0.6208 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7161 - val_loss: 8.9636 - val_gender_output_loss: 0.7271 - val_image_quality_output_loss: 0.9085 - val_age_output_loss: 1.4399 - val_weight_output_loss: 0.9944 - val_bag_output_loss: 0.9544 - val_footwear_output_loss: 1.5456 - val_pose_output_loss: 0.9396 - val_emotion_output_loss: 0.9616 - val_gender_output_acc: 0.6149 - val_image_quality_output_acc: 0.5464 - val_age_output_acc: 0.3664 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5398 - val_footwear_output_acc: 0.3982 - val_pose_output_acc: 0.6134 - val_emotion_output_acc: 0.6815\n",
            "Epoch 39/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 175s 487ms/step - loss: 7.7870 - gender_output_loss: 0.5510 - image_quality_output_loss: 0.8710 - age_output_loss: 1.4035 - weight_output_loss: 0.9748 - bag_output_loss: 0.8703 - footwear_output_loss: 0.8341 - pose_output_loss: 0.9119 - emotion_output_loss: 0.8813 - gender_output_acc: 0.7095 - image_quality_output_acc: 0.5788 - age_output_acc: 0.4039 - weight_output_acc: 0.6346 - bag_output_acc: 0.5952 - footwear_output_acc: 0.6263 - pose_output_acc: 0.6188 - emotion_output_acc: 0.7161 - val_loss: 8.2597 - val_gender_output_loss: 0.6118 - val_image_quality_output_loss: 1.1075 - val_age_output_loss: 1.4299 - val_weight_output_loss: 0.9815 - val_bag_output_loss: 0.9023 - val_footwear_output_loss: 0.8580 - val_pose_output_loss: 0.9256 - val_emotion_output_loss: 0.9572 - val_gender_output_acc: 0.7031 - val_image_quality_output_acc: 0.4627 - val_age_output_acc: 0.3770 - val_weight_output_acc: 0.6386 - val_bag_output_acc: 0.5781 - val_footwear_output_acc: 0.5882 - val_pose_output_acc: 0.6129 - val_emotion_output_acc: 0.6830\n",
            "Epoch 40/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 175s 487ms/step - loss: 7.7515 - gender_output_loss: 0.5368 - image_quality_output_loss: 0.8693 - age_output_loss: 1.3997 - weight_output_loss: 0.9741 - bag_output_loss: 0.8659 - footwear_output_loss: 0.8292 - pose_output_loss: 0.9116 - emotion_output_loss: 0.8821 - gender_output_acc: 0.7286 - image_quality_output_acc: 0.5819 - age_output_acc: 0.4044 - weight_output_acc: 0.6341 - bag_output_acc: 0.5971 - footwear_output_acc: 0.6293 - pose_output_acc: 0.6187 - emotion_output_acc: 0.7161 - val_loss: 9.9012 - val_gender_output_loss: 0.7424 - val_image_quality_output_loss: 0.9594 - val_age_output_loss: 1.4388 - val_weight_output_loss: 0.9988 - val_bag_output_loss: 0.9239 - val_footwear_output_loss: 2.4650 - val_pose_output_loss: 0.9323 - val_emotion_output_loss: 0.9609 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5237 - val_age_output_acc: 0.3674 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5509 - val_footwear_output_acc: 0.3730 - val_pose_output_acc: 0.6119 - val_emotion_output_acc: 0.6845\n",
            "Epoch 41/50\n",
            "Learning rate:  0.1\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.7242 - gender_output_loss: 0.5298 - image_quality_output_loss: 0.8677 - age_output_loss: 1.4008 - weight_output_loss: 0.9746 - bag_output_loss: 0.8629 - footwear_output_loss: 0.8231 - pose_output_loss: 0.9086 - emotion_output_loss: 0.8800 - gender_output_acc: 0.7323 - image_quality_output_acc: 0.5867 - age_output_acc: 0.4019 - weight_output_acc: 0.6344 - bag_output_acc: 0.6013 - footwear_output_acc: 0.6265 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7158Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 488ms/step - loss: 7.7230 - gender_output_loss: 0.5294 - image_quality_output_loss: 0.8678 - age_output_loss: 1.4005 - weight_output_loss: 0.9744 - bag_output_loss: 0.8625 - footwear_output_loss: 0.8229 - pose_output_loss: 0.9084 - emotion_output_loss: 0.8804 - gender_output_acc: 0.7328 - image_quality_output_acc: 0.5869 - age_output_acc: 0.4023 - weight_output_acc: 0.6345 - bag_output_acc: 0.6017 - footwear_output_acc: 0.6265 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7156 - val_loss: 8.2059 - val_gender_output_loss: 0.6475 - val_image_quality_output_loss: 0.9583 - val_age_output_loss: 1.4477 - val_weight_output_loss: 0.9905 - val_bag_output_loss: 0.9263 - val_footwear_output_loss: 0.8869 - val_pose_output_loss: 0.9177 - val_emotion_output_loss: 0.9573 - val_gender_output_acc: 0.6190 - val_image_quality_output_acc: 0.5580 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5454 - val_footwear_output_acc: 0.5907 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 42/50\n",
            "Learning rate:  0.3\n",
            "360/360 [==============================] - 175s 486ms/step - loss: 7.8983 - gender_output_loss: 0.5868 - image_quality_output_loss: 0.8950 - age_output_loss: 1.4092 - weight_output_loss: 0.9804 - bag_output_loss: 0.8850 - footwear_output_loss: 0.8705 - pose_output_loss: 0.9163 - emotion_output_loss: 0.8889 - gender_output_acc: 0.6837 - image_quality_output_acc: 0.5691 - age_output_acc: 0.4038 - weight_output_acc: 0.6344 - bag_output_acc: 0.5819 - footwear_output_acc: 0.6036 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7161 - val_loss: 9.3170 - val_gender_output_loss: 1.0544 - val_image_quality_output_loss: 0.9323 - val_age_output_loss: 1.4788 - val_weight_output_loss: 1.0092 - val_bag_output_loss: 1.0091 - val_footwear_output_loss: 1.4199 - val_pose_output_loss: 0.9560 - val_emotion_output_loss: 0.9991 - val_gender_output_acc: 0.5015 - val_image_quality_output_acc: 0.5242 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.4607 - val_footwear_output_acc: 0.4385 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 43/50\n",
            "Learning rate:  0.3\n",
            "360/360 [==============================] - 175s 487ms/step - loss: 7.7977 - gender_output_loss: 0.5587 - image_quality_output_loss: 0.8881 - age_output_loss: 1.4074 - weight_output_loss: 0.9779 - bag_output_loss: 0.8720 - footwear_output_loss: 0.8454 - pose_output_loss: 0.9109 - emotion_output_loss: 0.8867 - gender_output_acc: 0.7082 - image_quality_output_acc: 0.5726 - age_output_acc: 0.4003 - weight_output_acc: 0.6347 - bag_output_acc: 0.5911 - footwear_output_acc: 0.6182 - pose_output_acc: 0.6171 - emotion_output_acc: 0.7161 - val_loss: 10.7502 - val_gender_output_loss: 0.8649 - val_image_quality_output_loss: 0.9531 - val_age_output_loss: 1.4428 - val_weight_output_loss: 1.0034 - val_bag_output_loss: 0.9316 - val_footwear_output_loss: 3.1383 - val_pose_output_loss: 0.9938 - val_emotion_output_loss: 0.9791 - val_gender_output_acc: 0.4390 - val_image_quality_output_acc: 0.5393 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5484 - val_footwear_output_acc: 0.3649 - val_pose_output_acc: 0.6159 - val_emotion_output_acc: 0.6845\n",
            "Epoch 44/50\n",
            "Learning rate:  0.3\n",
            "360/360 [==============================] - 175s 487ms/step - loss: 7.7352 - gender_output_loss: 0.5427 - image_quality_output_loss: 0.8860 - age_output_loss: 1.4024 - weight_output_loss: 0.9774 - bag_output_loss: 0.8694 - footwear_output_loss: 0.8339 - pose_output_loss: 0.9034 - emotion_output_loss: 0.8838 - gender_output_acc: 0.7245 - image_quality_output_acc: 0.5713 - age_output_acc: 0.4019 - weight_output_acc: 0.6345 - bag_output_acc: 0.5993 - footwear_output_acc: 0.6311 - pose_output_acc: 0.6162 - emotion_output_acc: 0.7159 - val_loss: 10.4398 - val_gender_output_loss: 0.6311 - val_image_quality_output_loss: 1.0032 - val_age_output_loss: 1.5169 - val_weight_output_loss: 0.9856 - val_bag_output_loss: 0.9449 - val_footwear_output_loss: 2.9616 - val_pose_output_loss: 0.9946 - val_emotion_output_loss: 0.9729 - val_gender_output_acc: 0.6321 - val_image_quality_output_acc: 0.5287 - val_age_output_acc: 0.3574 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.3599 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 45/50\n",
            "Learning rate:  0.3\n",
            "360/360 [==============================] - 175s 486ms/step - loss: 7.6692 - gender_output_loss: 0.5247 - image_quality_output_loss: 0.8858 - age_output_loss: 1.4025 - weight_output_loss: 0.9765 - bag_output_loss: 0.8649 - footwear_output_loss: 0.8203 - pose_output_loss: 0.8883 - emotion_output_loss: 0.8836 - gender_output_acc: 0.7385 - image_quality_output_acc: 0.5706 - age_output_acc: 0.4002 - weight_output_acc: 0.6344 - bag_output_acc: 0.6009 - footwear_output_acc: 0.6319 - pose_output_acc: 0.6187 - emotion_output_acc: 0.7161 - val_loss: 9.1367 - val_gender_output_loss: 0.7273 - val_image_quality_output_loss: 1.2001 - val_age_output_loss: 1.5033 - val_weight_output_loss: 1.0046 - val_bag_output_loss: 1.1668 - val_footwear_output_loss: 1.0275 - val_pose_output_loss: 1.1036 - val_emotion_output_loss: 0.9872 - val_gender_output_acc: 0.5897 - val_image_quality_output_acc: 0.4395 - val_age_output_acc: 0.3690 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.5252 - val_pose_output_acc: 0.4294 - val_emotion_output_acc: 0.6845\n",
            "Epoch 46/50\n",
            "Learning rate:  0.3\n",
            "360/360 [==============================] - 175s 487ms/step - loss: 7.5993 - gender_output_loss: 0.5082 - image_quality_output_loss: 0.8817 - age_output_loss: 1.4005 - weight_output_loss: 0.9777 - bag_output_loss: 0.8635 - footwear_output_loss: 0.8178 - pose_output_loss: 0.8597 - emotion_output_loss: 0.8796 - gender_output_acc: 0.7510 - image_quality_output_acc: 0.5721 - age_output_acc: 0.4022 - weight_output_acc: 0.6346 - bag_output_acc: 0.6026 - footwear_output_acc: 0.6360 - pose_output_acc: 0.6293 - emotion_output_acc: 0.7166 - val_loss: 10.2052 - val_gender_output_loss: 1.8397 - val_image_quality_output_loss: 1.1092 - val_age_output_loss: 1.7993 - val_weight_output_loss: 0.9944 - val_bag_output_loss: 0.9713 - val_footwear_output_loss: 1.1037 - val_pose_output_loss: 0.9679 - val_emotion_output_loss: 1.0150 - val_gender_output_acc: 0.4370 - val_image_quality_output_acc: 0.4194 - val_age_output_acc: 0.3412 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.4567 - val_footwear_output_acc: 0.3438 - val_pose_output_acc: 0.5373 - val_emotion_output_acc: 0.6845\n",
            "Epoch 47/50\n",
            "Learning rate:  0.3\n",
            "360/360 [==============================] - 175s 487ms/step - loss: 7.5227 - gender_output_loss: 0.4956 - image_quality_output_loss: 0.8827 - age_output_loss: 1.3993 - weight_output_loss: 0.9767 - bag_output_loss: 0.8613 - footwear_output_loss: 0.8081 - pose_output_loss: 0.8224 - emotion_output_loss: 0.8768 - gender_output_acc: 0.7568 - image_quality_output_acc: 0.5747 - age_output_acc: 0.4012 - weight_output_acc: 0.6345 - bag_output_acc: 0.6064 - footwear_output_acc: 0.6422 - pose_output_acc: 0.6374 - emotion_output_acc: 0.7161 - val_loss: 9.8877 - val_gender_output_loss: 0.8240 - val_image_quality_output_loss: 0.9960 - val_age_output_loss: 1.4665 - val_weight_output_loss: 0.9898 - val_bag_output_loss: 1.1139 - val_footwear_output_loss: 1.5995 - val_pose_output_loss: 1.4683 - val_emotion_output_loss: 1.0347 - val_gender_output_acc: 0.5897 - val_image_quality_output_acc: 0.5217 - val_age_output_acc: 0.3674 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4103 - val_pose_output_acc: 0.3644 - val_emotion_output_acc: 0.6845\n",
            "Epoch 48/50\n",
            "Learning rate:  0.3\n",
            "360/360 [==============================] - 175s 487ms/step - loss: 7.4604 - gender_output_loss: 0.4813 - image_quality_output_loss: 0.8856 - age_output_loss: 1.3983 - weight_output_loss: 0.9765 - bag_output_loss: 0.8603 - footwear_output_loss: 0.8056 - pose_output_loss: 0.7891 - emotion_output_loss: 0.8725 - gender_output_acc: 0.7655 - image_quality_output_acc: 0.5728 - age_output_acc: 0.4020 - weight_output_acc: 0.6346 - bag_output_acc: 0.6045 - footwear_output_acc: 0.6459 - pose_output_acc: 0.6480 - emotion_output_acc: 0.7158 - val_loss: 7.8402 - val_gender_output_loss: 0.5973 - val_image_quality_output_loss: 0.8925 - val_age_output_loss: 1.4309 - val_weight_output_loss: 0.9870 - val_bag_output_loss: 0.9039 - val_footwear_output_loss: 0.8707 - val_pose_output_loss: 0.8329 - val_emotion_output_loss: 0.9381 - val_gender_output_acc: 0.7137 - val_image_quality_output_acc: 0.5630 - val_age_output_acc: 0.3710 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5817 - val_footwear_output_acc: 0.6109 - val_pose_output_acc: 0.6326 - val_emotion_output_acc: 0.6845\n",
            "Epoch 49/50\n",
            "Learning rate:  0.3\n",
            "360/360 [==============================] - 175s 485ms/step - loss: 7.3951 - gender_output_loss: 0.4731 - image_quality_output_loss: 0.8807 - age_output_loss: 1.3962 - weight_output_loss: 0.9762 - bag_output_loss: 0.8588 - footwear_output_loss: 0.8060 - pose_output_loss: 0.7513 - emotion_output_loss: 0.8694 - gender_output_acc: 0.7777 - image_quality_output_acc: 0.5749 - age_output_acc: 0.4036 - weight_output_acc: 0.6344 - bag_output_acc: 0.6067 - footwear_output_acc: 0.6467 - pose_output_acc: 0.6603 - emotion_output_acc: 0.7162 - val_loss: 10.4338 - val_gender_output_loss: 0.8244 - val_image_quality_output_loss: 1.3519 - val_age_output_loss: 1.4765 - val_weight_output_loss: 1.0006 - val_bag_output_loss: 0.9666 - val_footwear_output_loss: 2.3460 - val_pose_output_loss: 1.0107 - val_emotion_output_loss: 1.0772 - val_gender_output_acc: 0.5932 - val_image_quality_output_acc: 0.3241 - val_age_output_acc: 0.3543 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5418 - val_footwear_output_acc: 0.3936 - val_pose_output_acc: 0.5882 - val_emotion_output_acc: 0.6109\n",
            "Epoch 50/50\n",
            "Learning rate:  0.3\n",
            "360/360 [==============================] - 175s 486ms/step - loss: 7.3147 - gender_output_loss: 0.4544 - image_quality_output_loss: 0.8768 - age_output_loss: 1.3947 - weight_output_loss: 0.9762 - bag_output_loss: 0.8567 - footwear_output_loss: 0.7965 - pose_output_loss: 0.7185 - emotion_output_loss: 0.8641 - gender_output_acc: 0.7876 - image_quality_output_acc: 0.5768 - age_output_acc: 0.4036 - weight_output_acc: 0.6344 - bag_output_acc: 0.6075 - footwear_output_acc: 0.6502 - pose_output_acc: 0.6823 - emotion_output_acc: 0.7163 - val_loss: 8.9992 - val_gender_output_loss: 0.7509 - val_image_quality_output_loss: 0.9445 - val_age_output_loss: 1.4298 - val_weight_output_loss: 1.0008 - val_bag_output_loss: 0.9687 - val_footwear_output_loss: 1.0388 - val_pose_output_loss: 1.5171 - val_emotion_output_loss: 0.9752 - val_gender_output_acc: 0.6527 - val_image_quality_output_acc: 0.5514 - val_age_output_acc: 0.3700 - val_weight_output_acc: 0.6159 - val_bag_output_acc: 0.5509 - val_footwear_output_acc: 0.5726 - val_pose_output_acc: 0.6134 - val_emotion_output_acc: 0.6845\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7ff318f8d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRuM0tUHzzRw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive \n",
        "from google.colab import auth \n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQcqUaZxz2yd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()                       \n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_Xo0UTUz4EX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e979c458-5cea-4333-ebc6-1828618dab4e"
      },
      "source": [
        "model.save('EIP4Assignment5.h5')\n",
        "model_file = drive.CreateFile({'title' : 'EIP4Assignment5.h5'})                       \n",
        "model_file.SetContentFile('EIP4Assignment5.h5')                      \n",
        "model_file.Upload()\n",
        "drive.CreateFile({'id': model_file.get('id')})"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GoogleDriveFile({'id': '1_WLWZNFY9LsAkG5Vr_NycLB3mx7J_GTD'})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXfIPp2j0FH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# After the runtime is suspended, run the below commands\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive \n",
        "from google.colab import auth \n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()                       \n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "file_obj = drive.CreateFile({'id': '1_WLWZNFY9LsAkG5Vr_NycLB3mx7J_GTD'})                       \n",
        "file_obj.GetContentFile('EIP4Assignment5.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7uHJcFG0Ruw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import loadtxt\n",
        "from keras.models import load_model\n",
        "model = load_model('EIP4Assignment5.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhq492RrDtLE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9ef90290-6a97-4a2c-d094-471a943162d0"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=SGD(lr=0.1, momentum=0.1),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=50,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 194s 539ms/step - loss: 7.0341 - gender_output_loss: 0.3703 - image_quality_output_loss: 0.8587 - age_output_loss: 1.3831 - weight_output_loss: 0.9720 - bag_output_loss: 0.8417 - footwear_output_loss: 0.7552 - pose_output_loss: 0.6282 - emotion_output_loss: 0.8536 - gender_output_acc: 0.8340 - image_quality_output_acc: 0.5877 - age_output_acc: 0.4067 - weight_output_acc: 0.6352 - bag_output_acc: 0.6213 - footwear_output_acc: 0.6694 - pose_output_acc: 0.7249 - emotion_output_acc: 0.7161 - val_loss: 7.8616 - val_gender_output_loss: 0.5916 - val_image_quality_output_loss: 0.8692 - val_age_output_loss: 1.4357 - val_weight_output_loss: 0.9852 - val_bag_output_loss: 0.9249 - val_footwear_output_loss: 0.9070 - val_pose_output_loss: 0.8448 - val_emotion_output_loss: 0.9341 - val_gender_output_acc: 0.7636 - val_image_quality_output_acc: 0.5872 - val_age_output_acc: 0.3710 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5736 - val_footwear_output_acc: 0.6003 - val_pose_output_acc: 0.6447 - val_emotion_output_acc: 0.6845\n",
            "Epoch 2/50\n",
            "Learning rate:  0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
            "  'skipping.' % (self.monitor), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "360/360 [==============================] - 176s 489ms/step - loss: 6.9527 - gender_output_loss: 0.3520 - image_quality_output_loss: 0.8532 - age_output_loss: 1.3813 - weight_output_loss: 0.9722 - bag_output_loss: 0.8363 - footwear_output_loss: 0.7443 - pose_output_loss: 0.5950 - emotion_output_loss: 0.8509 - gender_output_acc: 0.8475 - image_quality_output_acc: 0.5934 - age_output_acc: 0.4066 - weight_output_acc: 0.6342 - bag_output_acc: 0.6279 - footwear_output_acc: 0.6773 - pose_output_acc: 0.7418 - emotion_output_acc: 0.7166 - val_loss: 7.5393 - val_gender_output_loss: 0.4573 - val_image_quality_output_loss: 0.8862 - val_age_output_loss: 1.4374 - val_weight_output_loss: 0.9850 - val_bag_output_loss: 0.8986 - val_footwear_output_loss: 0.8181 - val_pose_output_loss: 0.7515 - val_emotion_output_loss: 0.9393 - val_gender_output_acc: 0.7898 - val_image_quality_output_acc: 0.5660 - val_age_output_acc: 0.3599 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5867 - val_footwear_output_acc: 0.6270 - val_pose_output_acc: 0.6512 - val_emotion_output_acc: 0.6845\n",
            "Epoch 3/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 489ms/step - loss: 6.8996 - gender_output_loss: 0.3334 - image_quality_output_loss: 0.8492 - age_output_loss: 1.3780 - weight_output_loss: 0.9711 - bag_output_loss: 0.8336 - footwear_output_loss: 0.7371 - pose_output_loss: 0.5848 - emotion_output_loss: 0.8478 - gender_output_acc: 0.8545 - image_quality_output_acc: 0.5940 - age_output_acc: 0.4072 - weight_output_acc: 0.6345 - bag_output_acc: 0.6284 - footwear_output_acc: 0.6786 - pose_output_acc: 0.7448 - emotion_output_acc: 0.7161 - val_loss: 7.7195 - val_gender_output_loss: 0.5083 - val_image_quality_output_loss: 1.0686 - val_age_output_loss: 1.4389 - val_weight_output_loss: 0.9819 - val_bag_output_loss: 0.9037 - val_footwear_output_loss: 0.8560 - val_pose_output_loss: 0.6730 - val_emotion_output_loss: 0.9258 - val_gender_output_acc: 0.7697 - val_image_quality_output_acc: 0.4793 - val_age_output_acc: 0.3609 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5756 - val_footwear_output_acc: 0.6305 - val_pose_output_acc: 0.7072 - val_emotion_output_acc: 0.6845\n",
            "Epoch 4/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 489ms/step - loss: 6.8616 - gender_output_loss: 0.3263 - image_quality_output_loss: 0.8479 - age_output_loss: 1.3788 - weight_output_loss: 0.9709 - bag_output_loss: 0.8330 - footwear_output_loss: 0.7345 - pose_output_loss: 0.5625 - emotion_output_loss: 0.8456 - gender_output_acc: 0.8565 - image_quality_output_acc: 0.5980 - age_output_acc: 0.4052 - weight_output_acc: 0.6348 - bag_output_acc: 0.6317 - footwear_output_acc: 0.6798 - pose_output_acc: 0.7566 - emotion_output_acc: 0.7166 - val_loss: 8.3217 - val_gender_output_loss: 0.8808 - val_image_quality_output_loss: 1.1040 - val_age_output_loss: 1.4537 - val_weight_output_loss: 0.9862 - val_bag_output_loss: 0.9592 - val_footwear_output_loss: 0.8636 - val_pose_output_loss: 0.7576 - val_emotion_output_loss: 0.9553 - val_gender_output_acc: 0.6326 - val_image_quality_output_acc: 0.4743 - val_age_output_acc: 0.3644 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5186 - val_footwear_output_acc: 0.5852 - val_pose_output_acc: 0.6870 - val_emotion_output_acc: 0.6734\n",
            "Epoch 5/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 488ms/step - loss: 6.8114 - gender_output_loss: 0.3132 - image_quality_output_loss: 0.8459 - age_output_loss: 1.3791 - weight_output_loss: 0.9713 - bag_output_loss: 0.8299 - footwear_output_loss: 0.7212 - pose_output_loss: 0.5450 - emotion_output_loss: 0.8449 - gender_output_acc: 0.8612 - image_quality_output_acc: 0.5966 - age_output_acc: 0.4056 - weight_output_acc: 0.6347 - bag_output_acc: 0.6321 - footwear_output_acc: 0.6861 - pose_output_acc: 0.7638 - emotion_output_acc: 0.7162 - val_loss: 7.8706 - val_gender_output_loss: 0.4675 - val_image_quality_output_loss: 0.9391 - val_age_output_loss: 1.4256 - val_weight_output_loss: 0.9788 - val_bag_output_loss: 0.8908 - val_footwear_output_loss: 1.1026 - val_pose_output_loss: 0.7641 - val_emotion_output_loss: 0.9419 - val_gender_output_acc: 0.7868 - val_image_quality_output_acc: 0.5615 - val_age_output_acc: 0.3705 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5963 - val_footwear_output_acc: 0.5534 - val_pose_output_acc: 0.6951 - val_emotion_output_acc: 0.6845\n",
            "Epoch 6/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 488ms/step - loss: 6.7751 - gender_output_loss: 0.3057 - image_quality_output_loss: 0.8439 - age_output_loss: 1.3740 - weight_output_loss: 0.9700 - bag_output_loss: 0.8248 - footwear_output_loss: 0.7218 - pose_output_loss: 0.5345 - emotion_output_loss: 0.8404 - gender_output_acc: 0.8698 - image_quality_output_acc: 0.5997 - age_output_acc: 0.4082 - weight_output_acc: 0.6347 - bag_output_acc: 0.6352 - footwear_output_acc: 0.6862 - pose_output_acc: 0.7694 - emotion_output_acc: 0.7169 - val_loss: 11.3258 - val_gender_output_loss: 2.5625 - val_image_quality_output_loss: 0.9218 - val_age_output_loss: 1.6396 - val_weight_output_loss: 1.0318 - val_bag_output_loss: 1.2758 - val_footwear_output_loss: 1.2979 - val_pose_output_loss: 1.2376 - val_emotion_output_loss: 0.9991 - val_gender_output_acc: 0.5827 - val_image_quality_output_acc: 0.5645 - val_age_output_acc: 0.2863 - val_weight_output_acc: 0.6174 - val_bag_output_acc: 0.5504 - val_footwear_output_acc: 0.5615 - val_pose_output_acc: 0.6603 - val_emotion_output_acc: 0.6578\n",
            "Epoch 7/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 489ms/step - loss: 6.7245 - gender_output_loss: 0.2872 - image_quality_output_loss: 0.8435 - age_output_loss: 1.3758 - weight_output_loss: 0.9703 - bag_output_loss: 0.8253 - footwear_output_loss: 0.7130 - pose_output_loss: 0.5114 - emotion_output_loss: 0.8384 - gender_output_acc: 0.8786 - image_quality_output_acc: 0.6012 - age_output_acc: 0.4069 - weight_output_acc: 0.6348 - bag_output_acc: 0.6364 - footwear_output_acc: 0.6921 - pose_output_acc: 0.7805 - emotion_output_acc: 0.7169 - val_loss: 9.8160 - val_gender_output_loss: 0.7858 - val_image_quality_output_loss: 0.9873 - val_age_output_loss: 1.4400 - val_weight_output_loss: 0.9936 - val_bag_output_loss: 0.9211 - val_footwear_output_loss: 2.4765 - val_pose_output_loss: 0.8769 - val_emotion_output_loss: 0.9751 - val_gender_output_acc: 0.5993 - val_image_quality_output_acc: 0.5086 - val_age_output_acc: 0.3574 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5670 - val_footwear_output_acc: 0.4007 - val_pose_output_acc: 0.6537 - val_emotion_output_acc: 0.6719\n",
            "Epoch 8/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 490ms/step - loss: 6.6864 - gender_output_loss: 0.2780 - image_quality_output_loss: 0.8396 - age_output_loss: 1.3725 - weight_output_loss: 0.9706 - bag_output_loss: 0.8228 - footwear_output_loss: 0.7063 - pose_output_loss: 0.4997 - emotion_output_loss: 0.8370 - gender_output_acc: 0.8815 - image_quality_output_acc: 0.5996 - age_output_acc: 0.4085 - weight_output_acc: 0.6345 - bag_output_acc: 0.6332 - footwear_output_acc: 0.6949 - pose_output_acc: 0.7856 - emotion_output_acc: 0.7161 - val_loss: 8.7971 - val_gender_output_loss: 1.0472 - val_image_quality_output_loss: 1.3883 - val_age_output_loss: 1.4335 - val_weight_output_loss: 0.9874 - val_bag_output_loss: 0.9531 - val_footwear_output_loss: 0.8467 - val_pose_output_loss: 0.8319 - val_emotion_output_loss: 0.9490 - val_gender_output_acc: 0.6547 - val_image_quality_output_acc: 0.3543 - val_age_output_acc: 0.3695 - val_weight_output_acc: 0.6391 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.6255 - val_pose_output_acc: 0.6588 - val_emotion_output_acc: 0.6799\n",
            "Epoch 9/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 490ms/step - loss: 6.6705 - gender_output_loss: 0.2758 - image_quality_output_loss: 0.8391 - age_output_loss: 1.3744 - weight_output_loss: 0.9700 - bag_output_loss: 0.8256 - footwear_output_loss: 0.6973 - pose_output_loss: 0.4933 - emotion_output_loss: 0.8343 - gender_output_acc: 0.8861 - image_quality_output_acc: 0.6031 - age_output_acc: 0.4094 - weight_output_acc: 0.6348 - bag_output_acc: 0.6319 - footwear_output_acc: 0.6971 - pose_output_acc: 0.7862 - emotion_output_acc: 0.7162 - val_loss: 8.6595 - val_gender_output_loss: 0.6900 - val_image_quality_output_loss: 0.9244 - val_age_output_loss: 1.4304 - val_weight_output_loss: 0.9826 - val_bag_output_loss: 0.9476 - val_footwear_output_loss: 1.2037 - val_pose_output_loss: 1.1683 - val_emotion_output_loss: 0.9511 - val_gender_output_acc: 0.7152 - val_image_quality_output_acc: 0.5565 - val_age_output_acc: 0.3705 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5297 - val_footwear_output_acc: 0.5323 - val_pose_output_acc: 0.6573 - val_emotion_output_acc: 0.6749\n",
            "Epoch 10/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 489ms/step - loss: 6.6264 - gender_output_loss: 0.2709 - image_quality_output_loss: 0.8357 - age_output_loss: 1.3716 - weight_output_loss: 0.9693 - bag_output_loss: 0.8208 - footwear_output_loss: 0.6924 - pose_output_loss: 0.4677 - emotion_output_loss: 0.8358 - gender_output_acc: 0.8871 - image_quality_output_acc: 0.6050 - age_output_acc: 0.4110 - weight_output_acc: 0.6345 - bag_output_acc: 0.6358 - footwear_output_acc: 0.6956 - pose_output_acc: 0.8017 - emotion_output_acc: 0.7159 - val_loss: 9.2841 - val_gender_output_loss: 0.8866 - val_image_quality_output_loss: 1.1807 - val_age_output_loss: 1.4327 - val_weight_output_loss: 0.9899 - val_bag_output_loss: 0.9057 - val_footwear_output_loss: 1.7378 - val_pose_output_loss: 0.8096 - val_emotion_output_loss: 0.9783 - val_gender_output_acc: 0.6552 - val_image_quality_output_acc: 0.4551 - val_age_output_acc: 0.3664 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5721 - val_footwear_output_acc: 0.4546 - val_pose_output_acc: 0.7056 - val_emotion_output_acc: 0.6452\n",
            "Epoch 11/50\n",
            "Learning rate:  0.1\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.5983 - gender_output_loss: 0.2632 - image_quality_output_loss: 0.8309 - age_output_loss: 1.3703 - weight_output_loss: 0.9681 - bag_output_loss: 0.8177 - footwear_output_loss: 0.6871 - pose_output_loss: 0.4618 - emotion_output_loss: 0.8353 - gender_output_acc: 0.8898 - image_quality_output_acc: 0.6069 - age_output_acc: 0.4103 - weight_output_acc: 0.6347 - bag_output_acc: 0.6355 - footwear_output_acc: 0.6972 - pose_output_acc: 0.8041 - emotion_output_acc: 0.7161Epoch 11/50\n",
            "360/360 [==============================] - 176s 489ms/step - loss: 6.6264 - gender_output_loss: 0.2709 - image_quality_output_loss: 0.8357 - age_output_loss: 1.3716 - weight_output_loss: 0.9693 - bag_output_loss: 0.8208 - footwear_output_loss: 0.6924 - pose_output_loss: 0.4677 - emotion_output_loss: 0.8358 - gender_output_acc: 0.8871 - image_quality_output_acc: 0.6050 - age_output_acc: 0.4110 - weight_output_acc: 0.6345 - bag_output_acc: 0.6358 - footwear_output_acc: 0.6956 - pose_output_acc: 0.8017 - emotion_output_acc: 0.7159 - val_loss: 9.2841 - val_gender_output_loss: 0.8866 - val_image_quality_output_loss: 1.1807 - val_age_output_loss: 1.4327 - val_weight_output_loss: 0.9899 - val_bag_output_loss: 0.9057 - val_footwear_output_loss: 1.7378 - val_pose_output_loss: 0.8096 - val_emotion_output_loss: 0.9783 - val_gender_output_acc: 0.6552 - val_image_quality_output_acc: 0.4551 - val_age_output_acc: 0.3664 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5721 - val_footwear_output_acc: 0.4546 - val_pose_output_acc: 0.7056 - val_emotion_output_acc: 0.6452\n",
            "360/360 [==============================] - 176s 490ms/step - loss: 6.5976 - gender_output_loss: 0.2630 - image_quality_output_loss: 0.8312 - age_output_loss: 1.3700 - weight_output_loss: 0.9680 - bag_output_loss: 0.8180 - footwear_output_loss: 0.6871 - pose_output_loss: 0.4613 - emotion_output_loss: 0.8350 - gender_output_acc: 0.8899 - image_quality_output_acc: 0.6063 - age_output_acc: 0.4106 - weight_output_acc: 0.6348 - bag_output_acc: 0.6355 - footwear_output_acc: 0.6970 - pose_output_acc: 0.8044 - emotion_output_acc: 0.7163 - val_loss: 10.2598 - val_gender_output_loss: 2.0961 - val_image_quality_output_loss: 0.9153 - val_age_output_loss: 1.5791 - val_weight_output_loss: 1.0138 - val_bag_output_loss: 1.1276 - val_footwear_output_loss: 0.9769 - val_pose_output_loss: 1.2222 - val_emotion_output_loss: 0.9636 - val_gender_output_acc: 0.5907 - val_image_quality_output_acc: 0.5549 - val_age_output_acc: 0.3231 - val_weight_output_acc: 0.6356 - val_bag_output_acc: 0.5534 - val_footwear_output_acc: 0.6114 - val_pose_output_acc: 0.6326 - val_emotion_output_acc: 0.6845\n",
            "Epoch 12/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 490ms/step - loss: 6.5494 - gender_output_loss: 0.2497 - image_quality_output_loss: 0.8305 - age_output_loss: 1.3709 - weight_output_loss: 0.9676 - bag_output_loss: 0.8148 - footwear_output_loss: 0.6710 - pose_output_loss: 0.4454 - emotion_output_loss: 0.8333 - gender_output_acc: 0.8990 - image_quality_output_acc: 0.6022 - age_output_acc: 0.4147 - weight_output_acc: 0.6346 - bag_output_acc: 0.6424 - footwear_output_acc: 0.7071 - pose_output_acc: 0.8079 - emotion_output_acc: 0.7159 - val_loss: 10.5317 - val_gender_output_loss: 0.6396 - val_image_quality_output_loss: 0.9453 - val_age_output_loss: 1.4748 - val_weight_output_loss: 0.9944 - val_bag_output_loss: 0.9139 - val_footwear_output_loss: 3.1997 - val_pose_output_loss: 0.9952 - val_emotion_output_loss: 1.0014 - val_gender_output_acc: 0.6653 - val_image_quality_output_acc: 0.5307 - val_age_output_acc: 0.3412 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5696 - val_footwear_output_acc: 0.3800 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 13/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 177s 491ms/step - loss: 6.5308 - gender_output_loss: 0.2490 - image_quality_output_loss: 0.8267 - age_output_loss: 1.3734 - weight_output_loss: 0.9688 - bag_output_loss: 0.8109 - footwear_output_loss: 0.6680 - pose_output_loss: 0.4338 - emotion_output_loss: 0.8314 - gender_output_acc: 0.8970 - image_quality_output_acc: 0.6069 - age_output_acc: 0.4116 - weight_output_acc: 0.6347 - bag_output_acc: 0.6426 - footwear_output_acc: 0.7080 - pose_output_acc: 0.8184 - emotion_output_acc: 0.7169 - val_loss: 8.2320 - val_gender_output_loss: 0.5299 - val_image_quality_output_loss: 0.8809 - val_age_output_loss: 1.4275 - val_weight_output_loss: 0.9793 - val_bag_output_loss: 0.8975 - val_footwear_output_loss: 1.2016 - val_pose_output_loss: 0.9549 - val_emotion_output_loss: 0.9902 - val_gender_output_acc: 0.7374 - val_image_quality_output_acc: 0.5655 - val_age_output_acc: 0.3654 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5852 - val_footwear_output_acc: 0.5171 - val_pose_output_acc: 0.6532 - val_emotion_output_acc: 0.6845\n",
            "Epoch 14/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 177s 491ms/step - loss: 6.4948 - gender_output_loss: 0.2426 - image_quality_output_loss: 0.8230 - age_output_loss: 1.3716 - weight_output_loss: 0.9686 - bag_output_loss: 0.8098 - footwear_output_loss: 0.6619 - pose_output_loss: 0.4166 - emotion_output_loss: 0.8292 - gender_output_acc: 0.9002 - image_quality_output_acc: 0.6083 - age_output_acc: 0.4083 - weight_output_acc: 0.6348 - bag_output_acc: 0.6438 - footwear_output_acc: 0.7120 - pose_output_acc: 0.8244 - emotion_output_acc: 0.7163 - val_loss: 12.7440 - val_gender_output_loss: 2.4842 - val_image_quality_output_loss: 1.2083 - val_age_output_loss: 1.8141 - val_weight_output_loss: 1.0525 - val_bag_output_loss: 1.1693 - val_footwear_output_loss: 1.5528 - val_pose_output_loss: 2.0985 - val_emotion_output_loss: 0.9915 - val_gender_output_acc: 0.6633 - val_image_quality_output_acc: 0.4521 - val_age_output_acc: 0.2208 - val_weight_output_acc: 0.5237 - val_bag_output_acc: 0.5796 - val_footwear_output_acc: 0.5287 - val_pose_output_acc: 0.6411 - val_emotion_output_acc: 0.6507\n",
            "Epoch 15/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 489ms/step - loss: 6.4754 - gender_output_loss: 0.2420 - image_quality_output_loss: 0.8203 - age_output_loss: 1.3702 - weight_output_loss: 0.9669 - bag_output_loss: 0.8080 - footwear_output_loss: 0.6547 - pose_output_loss: 0.4069 - emotion_output_loss: 0.8317 - gender_output_acc: 0.9002 - image_quality_output_acc: 0.6092 - age_output_acc: 0.4102 - weight_output_acc: 0.6346 - bag_output_acc: 0.6484 - footwear_output_acc: 0.7140 - pose_output_acc: 0.8311 - emotion_output_acc: 0.7174 - val_loss: 10.0966 - val_gender_output_loss: 2.0890 - val_image_quality_output_loss: 0.9069 - val_age_output_loss: 1.4647 - val_weight_output_loss: 1.0090 - val_bag_output_loss: 1.1162 - val_footwear_output_loss: 0.9016 - val_pose_output_loss: 1.2755 - val_emotion_output_loss: 0.9572 - val_gender_output_acc: 0.4607 - val_image_quality_output_acc: 0.5554 - val_age_output_acc: 0.3664 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.3886 - val_footwear_output_acc: 0.5917 - val_pose_output_acc: 0.6709 - val_emotion_output_acc: 0.6734\n",
            "Epoch 16/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 488ms/step - loss: 6.4302 - gender_output_loss: 0.2236 - image_quality_output_loss: 0.8172 - age_output_loss: 1.3719 - weight_output_loss: 0.9677 - bag_output_loss: 0.8120 - footwear_output_loss: 0.6415 - pose_output_loss: 0.3913 - emotion_output_loss: 0.8270 - gender_output_acc: 0.9081 - image_quality_output_acc: 0.6074 - age_output_acc: 0.4109 - weight_output_acc: 0.6345 - bag_output_acc: 0.6439 - footwear_output_acc: 0.7252 - pose_output_acc: 0.8369 - emotion_output_acc: 0.7165 - val_loss: 14.0879 - val_gender_output_loss: 1.7449 - val_image_quality_output_loss: 2.0838 - val_age_output_loss: 1.4975 - val_weight_output_loss: 1.0383 - val_bag_output_loss: 1.2231 - val_footwear_output_loss: 1.1007 - val_pose_output_loss: 3.8169 - val_emotion_output_loss: 1.2026 - val_gender_output_acc: 0.5312 - val_image_quality_output_acc: 0.3029 - val_age_output_acc: 0.3654 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.3977 - val_footwear_output_acc: 0.5086 - val_pose_output_acc: 0.3503 - val_emotion_output_acc: 0.6129\n",
            "Epoch 17/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 489ms/step - loss: 6.4091 - gender_output_loss: 0.2336 - image_quality_output_loss: 0.8124 - age_output_loss: 1.3678 - weight_output_loss: 0.9678 - bag_output_loss: 0.8060 - footwear_output_loss: 0.6396 - pose_output_loss: 0.3749 - emotion_output_loss: 0.8251 - gender_output_acc: 0.9023 - image_quality_output_acc: 0.6117 - age_output_acc: 0.4096 - weight_output_acc: 0.6345 - bag_output_acc: 0.6503 - footwear_output_acc: 0.7200 - pose_output_acc: 0.8455 - emotion_output_acc: 0.7161 - val_loss: 10.0574 - val_gender_output_loss: 0.7200 - val_image_quality_output_loss: 0.9333 - val_age_output_loss: 1.4445 - val_weight_output_loss: 0.9947 - val_bag_output_loss: 1.0438 - val_footwear_output_loss: 2.0050 - val_pose_output_loss: 1.5832 - val_emotion_output_loss: 0.9490 - val_gender_output_acc: 0.7550 - val_image_quality_output_acc: 0.5549 - val_age_output_acc: 0.3735 - val_weight_output_acc: 0.6391 - val_bag_output_acc: 0.5509 - val_footwear_output_acc: 0.3947 - val_pose_output_acc: 0.3574 - val_emotion_output_acc: 0.6835\n",
            "Epoch 18/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 488ms/step - loss: 6.3584 - gender_output_loss: 0.2088 - image_quality_output_loss: 0.8063 - age_output_loss: 1.3699 - weight_output_loss: 0.9676 - bag_output_loss: 0.8064 - footwear_output_loss: 0.6205 - pose_output_loss: 0.3685 - emotion_output_loss: 0.8247 - gender_output_acc: 0.9140 - image_quality_output_acc: 0.6194 - age_output_acc: 0.4110 - weight_output_acc: 0.6346 - bag_output_acc: 0.6480 - footwear_output_acc: 0.7294 - pose_output_acc: 0.8444 - emotion_output_acc: 0.7169 - val_loss: 10.1757 - val_gender_output_loss: 1.9349 - val_image_quality_output_loss: 1.2234 - val_age_output_loss: 1.6440 - val_weight_output_loss: 1.0145 - val_bag_output_loss: 1.1186 - val_footwear_output_loss: 1.0233 - val_pose_output_loss: 0.8484 - val_emotion_output_loss: 0.9808 - val_gender_output_acc: 0.6053 - val_image_quality_output_acc: 0.4662 - val_age_output_acc: 0.3231 - val_weight_output_acc: 0.6300 - val_bag_output_acc: 0.5534 - val_footwear_output_acc: 0.6043 - val_pose_output_acc: 0.7016 - val_emotion_output_acc: 0.6845\n",
            "Epoch 19/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 489ms/step - loss: 6.3675 - gender_output_loss: 0.2189 - image_quality_output_loss: 0.8010 - age_output_loss: 1.3708 - weight_output_loss: 0.9675 - bag_output_loss: 0.8081 - footwear_output_loss: 0.6221 - pose_output_loss: 0.3647 - emotion_output_loss: 0.8244 - gender_output_acc: 0.9131 - image_quality_output_acc: 0.6241 - age_output_acc: 0.4128 - weight_output_acc: 0.6349 - bag_output_acc: 0.6464 - footwear_output_acc: 0.7267 - pose_output_acc: 0.8500 - emotion_output_acc: 0.7174 - val_loss: 13.4147 - val_gender_output_loss: 0.6301 - val_image_quality_output_loss: 1.4200 - val_age_output_loss: 1.4318 - val_weight_output_loss: 0.9928 - val_bag_output_loss: 0.8913 - val_footwear_output_loss: 5.2819 - val_pose_output_loss: 1.3627 - val_emotion_output_loss: 1.0120 - val_gender_output_acc: 0.7182 - val_image_quality_output_acc: 0.3624 - val_age_output_acc: 0.3634 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5887 - val_footwear_output_acc: 0.3720 - val_pose_output_acc: 0.6512 - val_emotion_output_acc: 0.6739\n",
            "Epoch 20/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 489ms/step - loss: 6.3412 - gender_output_loss: 0.2184 - image_quality_output_loss: 0.7949 - age_output_loss: 1.3687 - weight_output_loss: 0.9653 - bag_output_loss: 0.8080 - footwear_output_loss: 0.6187 - pose_output_loss: 0.3490 - emotion_output_loss: 0.8240 - gender_output_acc: 0.9119 - image_quality_output_acc: 0.6237 - age_output_acc: 0.4109 - weight_output_acc: 0.6342 - bag_output_acc: 0.6452 - footwear_output_acc: 0.7229 - pose_output_acc: 0.8538 - emotion_output_acc: 0.7167 - val_loss: 14.0392 - val_gender_output_loss: 4.2069 - val_image_quality_output_loss: 1.3350 - val_age_output_loss: 1.5520 - val_weight_output_loss: 1.1184 - val_bag_output_loss: 1.4272 - val_footwear_output_loss: 1.4112 - val_pose_output_loss: 1.5492 - val_emotion_output_loss: 1.0426 - val_gender_output_acc: 0.5811 - val_image_quality_output_acc: 0.4375 - val_age_output_acc: 0.2697 - val_weight_output_acc: 0.5408 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4642 - val_pose_output_acc: 0.5227 - val_emotion_output_acc: 0.6517\n",
            "Epoch 21/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 175s 487ms/step - loss: 6.2916 - gender_output_loss: 0.2142 - image_quality_output_loss: 0.7887 - age_output_loss: 1.3675 - weight_output_loss: 0.9656 - bag_output_loss: 0.8025 - footwear_output_loss: 0.6006 - pose_output_loss: 0.3301 - emotion_output_loss: 0.8237 - gender_output_acc: 0.9140 - image_quality_output_acc: 0.6277 - age_output_acc: 0.4126 - weight_output_acc: 0.6354 - bag_output_acc: 0.6470 - footwear_output_acc: 0.7370 - pose_output_acc: 0.8612 - emotion_output_acc: 0.7167 - val_loss: 11.5269 - val_gender_output_loss: 1.3823 - val_image_quality_output_loss: 0.9860 - val_age_output_loss: 1.4902 - val_weight_output_loss: 0.9937 - val_bag_output_loss: 0.9862 - val_footwear_output_loss: 2.5794 - val_pose_output_loss: 1.6846 - val_emotion_output_loss: 1.0234 - val_gender_output_acc: 0.6598 - val_image_quality_output_acc: 0.5544 - val_age_output_acc: 0.3614 - val_weight_output_acc: 0.6376 - val_bag_output_acc: 0.5227 - val_footwear_output_acc: 0.4491 - val_pose_output_acc: 0.6638 - val_emotion_output_acc: 0.6830\n",
            "Epoch 22/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 175s 487ms/step - loss: 6.2710 - gender_output_loss: 0.2012 - image_quality_output_loss: 0.7848 - age_output_loss: 1.3701 - weight_output_loss: 0.9658 - bag_output_loss: 0.8049 - footwear_output_loss: 0.5905 - pose_output_loss: 0.3270 - emotion_output_loss: 0.8233 - gender_output_acc: 0.9187 - image_quality_output_acc: 0.6299 - age_output_acc: 0.4130 - weight_output_acc: 0.6346 - bag_output_acc: 0.6446 - footwear_output_acc: 0.7403 - pose_output_acc: 0.8676 - emotion_output_acc: 0.7163 - val_loss: 10.6543 - val_gender_output_loss: 0.8088 - val_image_quality_output_loss: 0.9293 - val_age_output_loss: 1.4285 - val_weight_output_loss: 0.9918 - val_bag_output_loss: 0.9136 - val_footwear_output_loss: 2.2498 - val_pose_output_loss: 1.9470 - val_emotion_output_loss: 0.9796 - val_gender_output_acc: 0.6689 - val_image_quality_output_acc: 0.5373 - val_age_output_acc: 0.3735 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5605 - val_footwear_output_acc: 0.4814 - val_pose_output_acc: 0.6356 - val_emotion_output_acc: 0.6734\n",
            "Epoch 23/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 175s 487ms/step - loss: 6.2421 - gender_output_loss: 0.1977 - image_quality_output_loss: 0.7677 - age_output_loss: 1.3677 - weight_output_loss: 0.9655 - bag_output_loss: 0.8011 - footwear_output_loss: 0.5900 - pose_output_loss: 0.3222 - emotion_output_loss: 0.8220 - gender_output_acc: 0.9194 - image_quality_output_acc: 0.6319 - age_output_acc: 0.4167 - weight_output_acc: 0.6348 - bag_output_acc: 0.6474 - footwear_output_acc: 0.7401 - pose_output_acc: 0.8714 - emotion_output_acc: 0.7163 - val_loss: 11.1049 - val_gender_output_loss: 1.2305 - val_image_quality_output_loss: 0.9506 - val_age_output_loss: 1.4602 - val_weight_output_loss: 0.9883 - val_bag_output_loss: 1.0916 - val_footwear_output_loss: 1.0402 - val_pose_output_loss: 2.8472 - val_emotion_output_loss: 1.0855 - val_gender_output_acc: 0.7001 - val_image_quality_output_acc: 0.5660 - val_age_output_acc: 0.3543 - val_weight_output_acc: 0.6406 - val_bag_output_acc: 0.5711 - val_footwear_output_acc: 0.6129 - val_pose_output_acc: 0.4062 - val_emotion_output_acc: 0.6845\n",
            "Epoch 24/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 175s 487ms/step - loss: 6.2444 - gender_output_loss: 0.2071 - image_quality_output_loss: 0.7654 - age_output_loss: 1.3676 - weight_output_loss: 0.9653 - bag_output_loss: 0.8034 - footwear_output_loss: 0.5867 - pose_output_loss: 0.3158 - emotion_output_loss: 0.8197 - gender_output_acc: 0.9164 - image_quality_output_acc: 0.6369 - age_output_acc: 0.4108 - weight_output_acc: 0.6349 - bag_output_acc: 0.6507 - footwear_output_acc: 0.7411 - pose_output_acc: 0.8756 - emotion_output_acc: 0.7163 - val_loss: 9.2393 - val_gender_output_loss: 1.1909 - val_image_quality_output_loss: 1.0183 - val_age_output_loss: 1.4668 - val_weight_output_loss: 0.9963 - val_bag_output_loss: 1.0464 - val_footwear_output_loss: 0.9152 - val_pose_output_loss: 1.2003 - val_emotion_output_loss: 0.9893 - val_gender_output_acc: 0.6603 - val_image_quality_output_acc: 0.5333 - val_age_output_acc: 0.3634 - val_weight_output_acc: 0.6406 - val_bag_output_acc: 0.5539 - val_footwear_output_acc: 0.6250 - val_pose_output_acc: 0.6174 - val_emotion_output_acc: 0.6845\n",
            "Epoch 25/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 488ms/step - loss: 6.1889 - gender_output_loss: 0.1866 - image_quality_output_loss: 0.7560 - age_output_loss: 1.3686 - weight_output_loss: 0.9644 - bag_output_loss: 0.8010 - footwear_output_loss: 0.5718 - pose_output_loss: 0.3031 - emotion_output_loss: 0.8189 - gender_output_acc: 0.9222 - image_quality_output_acc: 0.6425 - age_output_acc: 0.4143 - weight_output_acc: 0.6346 - bag_output_acc: 0.6512 - footwear_output_acc: 0.7466 - pose_output_acc: 0.8789 - emotion_output_acc: 0.7166 - val_loss: 11.8223 - val_gender_output_loss: 1.8509 - val_image_quality_output_loss: 1.0648 - val_age_output_loss: 1.5641 - val_weight_output_loss: 1.0163 - val_bag_output_loss: 1.1269 - val_footwear_output_loss: 1.6312 - val_pose_output_loss: 2.0581 - val_emotion_output_loss: 1.0888 - val_gender_output_acc: 0.5847 - val_image_quality_output_acc: 0.4965 - val_age_output_acc: 0.3372 - val_weight_output_acc: 0.6341 - val_bag_output_acc: 0.5494 - val_footwear_output_acc: 0.5040 - val_pose_output_acc: 0.4682 - val_emotion_output_acc: 0.6840\n",
            "Epoch 26/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 488ms/step - loss: 6.1755 - gender_output_loss: 0.1939 - image_quality_output_loss: 0.7401 - age_output_loss: 1.3668 - weight_output_loss: 0.9654 - bag_output_loss: 0.8015 - footwear_output_loss: 0.5704 - pose_output_loss: 0.2959 - emotion_output_loss: 0.8179 - gender_output_acc: 0.9222 - image_quality_output_acc: 0.6536 - age_output_acc: 0.4104 - weight_output_acc: 0.6345 - bag_output_acc: 0.6499 - footwear_output_acc: 0.7452 - pose_output_acc: 0.8800 - emotion_output_acc: 0.7170 - val_loss: 21.2260 - val_gender_output_loss: 6.4377 - val_image_quality_output_loss: 3.1304 - val_age_output_loss: 2.0931 - val_weight_output_loss: 1.0607 - val_bag_output_loss: 1.1476 - val_footwear_output_loss: 2.1413 - val_pose_output_loss: 3.6087 - val_emotion_output_loss: 1.1801 - val_gender_output_acc: 0.4360 - val_image_quality_output_acc: 0.2571 - val_age_output_acc: 0.2218 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.3483 - val_footwear_output_acc: 0.2112 - val_pose_output_acc: 0.2566 - val_emotion_output_acc: 0.5565\n",
            "Epoch 27/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 488ms/step - loss: 6.1492 - gender_output_loss: 0.1853 - image_quality_output_loss: 0.7362 - age_output_loss: 1.3681 - weight_output_loss: 0.9652 - bag_output_loss: 0.7967 - footwear_output_loss: 0.5527 - pose_output_loss: 0.2982 - emotion_output_loss: 0.8176 - gender_output_acc: 0.9240 - image_quality_output_acc: 0.6577 - age_output_acc: 0.4130 - weight_output_acc: 0.6347 - bag_output_acc: 0.6540 - footwear_output_acc: 0.7545 - pose_output_acc: 0.8837 - emotion_output_acc: 0.7174 - val_loss: 11.4474 - val_gender_output_loss: 2.0290 - val_image_quality_output_loss: 1.7537 - val_age_output_loss: 1.4684 - val_weight_output_loss: 1.0099 - val_bag_output_loss: 1.1571 - val_footwear_output_loss: 1.4680 - val_pose_output_loss: 1.0286 - val_emotion_output_loss: 1.1004 - val_gender_output_acc: 0.5358 - val_image_quality_output_acc: 0.4309 - val_age_output_acc: 0.3664 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.4189 - val_footwear_output_acc: 0.4803 - val_pose_output_acc: 0.7102 - val_emotion_output_acc: 0.5575\n",
            "Epoch 28/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 488ms/step - loss: 6.1375 - gender_output_loss: 0.1953 - image_quality_output_loss: 0.7261 - age_output_loss: 1.3652 - weight_output_loss: 0.9630 - bag_output_loss: 0.7988 - footwear_output_loss: 0.5479 - pose_output_loss: 0.2906 - emotion_output_loss: 0.8158 - gender_output_acc: 0.9205 - image_quality_output_acc: 0.6576 - age_output_acc: 0.4127 - weight_output_acc: 0.6348 - bag_output_acc: 0.6499 - footwear_output_acc: 0.7534 - pose_output_acc: 0.8860 - emotion_output_acc: 0.7168 - val_loss: 10.0081 - val_gender_output_loss: 0.4889 - val_image_quality_output_loss: 1.0110 - val_age_output_loss: 1.4473 - val_weight_output_loss: 0.9896 - val_bag_output_loss: 0.9149 - val_footwear_output_loss: 1.4784 - val_pose_output_loss: 2.1671 - val_emotion_output_loss: 1.0730 - val_gender_output_acc: 0.7969 - val_image_quality_output_acc: 0.5534 - val_age_output_acc: 0.3674 - val_weight_output_acc: 0.6391 - val_bag_output_acc: 0.5756 - val_footwear_output_acc: 0.5086 - val_pose_output_acc: 0.4204 - val_emotion_output_acc: 0.6845\n",
            "Epoch 29/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 489ms/step - loss: 6.0888 - gender_output_loss: 0.1777 - image_quality_output_loss: 0.7053 - age_output_loss: 1.3681 - weight_output_loss: 0.9644 - bag_output_loss: 0.7964 - footwear_output_loss: 0.5467 - pose_output_loss: 0.2725 - emotion_output_loss: 0.8171 - gender_output_acc: 0.9279 - image_quality_output_acc: 0.6721 - age_output_acc: 0.4125 - weight_output_acc: 0.6358 - bag_output_acc: 0.6517 - footwear_output_acc: 0.7567 - pose_output_acc: 0.8933 - emotion_output_acc: 0.7164 - val_loss: 10.1941 - val_gender_output_loss: 1.7010 - val_image_quality_output_loss: 1.5385 - val_age_output_loss: 1.5373 - val_weight_output_loss: 0.9866 - val_bag_output_loss: 0.9304 - val_footwear_output_loss: 0.9755 - val_pose_output_loss: 1.1184 - val_emotion_output_loss: 0.9627 - val_gender_output_acc: 0.5544 - val_image_quality_output_acc: 0.3982 - val_age_output_acc: 0.3644 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5433 - val_footwear_output_acc: 0.5645 - val_pose_output_acc: 0.6477 - val_emotion_output_acc: 0.6835\n",
            "Epoch 30/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 488ms/step - loss: 6.0977 - gender_output_loss: 0.1876 - image_quality_output_loss: 0.6975 - age_output_loss: 1.3647 - weight_output_loss: 0.9654 - bag_output_loss: 0.7954 - footwear_output_loss: 0.5411 - pose_output_loss: 0.2840 - emotion_output_loss: 0.8155 - gender_output_acc: 0.9236 - image_quality_output_acc: 0.6808 - age_output_acc: 0.4149 - weight_output_acc: 0.6346 - bag_output_acc: 0.6512 - footwear_output_acc: 0.7524 - pose_output_acc: 0.8866 - emotion_output_acc: 0.7153 - val_loss: 18.9258 - val_gender_output_loss: 4.8887 - val_image_quality_output_loss: 3.7410 - val_age_output_loss: 1.5890 - val_weight_output_loss: 1.0752 - val_bag_output_loss: 1.4140 - val_footwear_output_loss: 2.8748 - val_pose_output_loss: 1.2839 - val_emotion_output_loss: 1.6095 - val_gender_output_acc: 0.4405 - val_image_quality_output_acc: 0.3130 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.3498 - val_footwear_output_acc: 0.2026 - val_pose_output_acc: 0.5302 - val_emotion_output_acc: 0.2717\n",
            "Epoch 31/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 488ms/step - loss: 6.0421 - gender_output_loss: 0.1790 - image_quality_output_loss: 0.6769 - age_output_loss: 1.3644 - weight_output_loss: 0.9639 - bag_output_loss: 0.7943 - footwear_output_loss: 0.5345 - pose_output_loss: 0.2615 - emotion_output_loss: 0.8151 - gender_output_acc: 0.9308 - image_quality_output_acc: 0.6879 - age_output_acc: 0.4137 - weight_output_acc: 0.6351 - bag_output_acc: 0.6526 - footwear_output_acc: 0.7625 - pose_output_acc: 0.8989 - emotion_output_acc: 0.7156 - val_loss: 11.1209 - val_gender_output_loss: 0.7258 - val_image_quality_output_loss: 1.8169 - val_age_output_loss: 1.4558 - val_weight_output_loss: 0.9832 - val_bag_output_loss: 0.9239 - val_footwear_output_loss: 2.8058 - val_pose_output_loss: 1.0205 - val_emotion_output_loss: 0.9332 - val_gender_output_acc: 0.7233 - val_image_quality_output_acc: 0.3599 - val_age_output_acc: 0.3579 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5847 - val_footwear_output_acc: 0.4435 - val_pose_output_acc: 0.6275 - val_emotion_output_acc: 0.6845\n",
            "Epoch 32/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 175s 487ms/step - loss: 6.0196 - gender_output_loss: 0.1810 - image_quality_output_loss: 0.6623 - age_output_loss: 1.3631 - weight_output_loss: 0.9636 - bag_output_loss: 0.7896 - footwear_output_loss: 0.5217 - pose_output_loss: 0.2633 - emotion_output_loss: 0.8164 - gender_output_acc: 0.9283 - image_quality_output_acc: 0.6944 - age_output_acc: 0.4132 - weight_output_acc: 0.6352 - bag_output_acc: 0.6556 - footwear_output_acc: 0.7642 - pose_output_acc: 0.8969 - emotion_output_acc: 0.7173 - val_loss: 13.8973 - val_gender_output_loss: 2.6667 - val_image_quality_output_loss: 2.3736 - val_age_output_loss: 1.5073 - val_weight_output_loss: 1.0294 - val_bag_output_loss: 1.1725 - val_footwear_output_loss: 2.2519 - val_pose_output_loss: 1.3691 - val_emotion_output_loss: 1.0648 - val_gender_output_acc: 0.6416 - val_image_quality_output_acc: 0.3281 - val_age_output_acc: 0.3372 - val_weight_output_acc: 0.6270 - val_bag_output_acc: 0.5645 - val_footwear_output_acc: 0.5197 - val_pose_output_acc: 0.6492 - val_emotion_output_acc: 0.6179\n",
            "Epoch 33/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 488ms/step - loss: 5.9986 - gender_output_loss: 0.1693 - image_quality_output_loss: 0.6496 - age_output_loss: 1.3633 - weight_output_loss: 0.9639 - bag_output_loss: 0.7940 - footwear_output_loss: 0.5228 - pose_output_loss: 0.2582 - emotion_output_loss: 0.8126 - gender_output_acc: 0.9307 - image_quality_output_acc: 0.7005 - age_output_acc: 0.4145 - weight_output_acc: 0.6352 - bag_output_acc: 0.6517 - footwear_output_acc: 0.7641 - pose_output_acc: 0.9006 - emotion_output_acc: 0.7162 - val_loss: 14.1034 - val_gender_output_loss: 1.7895 - val_image_quality_output_loss: 2.6078 - val_age_output_loss: 1.4813 - val_weight_output_loss: 1.0117 - val_bag_output_loss: 1.0593 - val_footwear_output_loss: 3.1422 - val_pose_output_loss: 1.4381 - val_emotion_output_loss: 1.1052 - val_gender_output_acc: 0.6981 - val_image_quality_output_acc: 0.2863 - val_age_output_acc: 0.3498 - val_weight_output_acc: 0.6346 - val_bag_output_acc: 0.5721 - val_footwear_output_acc: 0.4365 - val_pose_output_acc: 0.6537 - val_emotion_output_acc: 0.5575\n",
            "Epoch 34/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 489ms/step - loss: 5.9667 - gender_output_loss: 0.1723 - image_quality_output_loss: 0.6281 - age_output_loss: 1.3619 - weight_output_loss: 0.9636 - bag_output_loss: 0.7896 - footwear_output_loss: 0.5196 - pose_output_loss: 0.2510 - emotion_output_loss: 0.8093 - gender_output_acc: 0.9313 - image_quality_output_acc: 0.7141 - age_output_acc: 0.4134 - weight_output_acc: 0.6351 - bag_output_acc: 0.6547 - footwear_output_acc: 0.7647 - pose_output_acc: 0.9013 - emotion_output_acc: 0.7174 - val_loss: 12.6055 - val_gender_output_loss: 1.4295 - val_image_quality_output_loss: 1.2443 - val_age_output_loss: 1.4750 - val_weight_output_loss: 1.0208 - val_bag_output_loss: 1.2524 - val_footwear_output_loss: 2.1364 - val_pose_output_loss: 2.4681 - val_emotion_output_loss: 1.1044 - val_gender_output_acc: 0.6058 - val_image_quality_output_acc: 0.5277 - val_age_output_acc: 0.3644 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.3800 - val_footwear_output_acc: 0.4693 - val_pose_output_acc: 0.6719 - val_emotion_output_acc: 0.6799\n",
            "Epoch 35/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 489ms/step - loss: 5.9602 - gender_output_loss: 0.1718 - image_quality_output_loss: 0.6197 - age_output_loss: 1.3615 - weight_output_loss: 0.9609 - bag_output_loss: 0.7887 - footwear_output_loss: 0.5173 - pose_output_loss: 0.2519 - emotion_output_loss: 0.8104 - gender_output_acc: 0.9312 - image_quality_output_acc: 0.7213 - age_output_acc: 0.4141 - weight_output_acc: 0.6357 - bag_output_acc: 0.6555 - footwear_output_acc: 0.7665 - pose_output_acc: 0.8995 - emotion_output_acc: 0.7176 - val_loss: 11.6598 - val_gender_output_loss: 1.7189 - val_image_quality_output_loss: 2.5325 - val_age_output_loss: 1.4321 - val_weight_output_loss: 0.9961 - val_bag_output_loss: 1.0036 - val_footwear_output_loss: 1.0676 - val_pose_output_loss: 1.4030 - val_emotion_output_loss: 1.0245 - val_gender_output_acc: 0.6603 - val_image_quality_output_acc: 0.3478 - val_age_output_acc: 0.3690 - val_weight_output_acc: 0.6376 - val_bag_output_acc: 0.5620 - val_footwear_output_acc: 0.5736 - val_pose_output_acc: 0.5529 - val_emotion_output_acc: 0.6164\n",
            "Epoch 36/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 488ms/step - loss: 5.9391 - gender_output_loss: 0.1695 - image_quality_output_loss: 0.6001 - age_output_loss: 1.3644 - weight_output_loss: 0.9634 - bag_output_loss: 0.7907 - footwear_output_loss: 0.5091 - pose_output_loss: 0.2488 - emotion_output_loss: 0.8086 - gender_output_acc: 0.9334 - image_quality_output_acc: 0.7298 - age_output_acc: 0.4130 - weight_output_acc: 0.6344 - bag_output_acc: 0.6534 - footwear_output_acc: 0.7707 - pose_output_acc: 0.9015 - emotion_output_acc: 0.7175 - val_loss: 13.8754 - val_gender_output_loss: 0.6155 - val_image_quality_output_loss: 2.1251 - val_age_output_loss: 1.4333 - val_weight_output_loss: 0.9908 - val_bag_output_loss: 0.9173 - val_footwear_output_loss: 5.6030 - val_pose_output_loss: 0.7569 - val_emotion_output_loss: 0.9453 - val_gender_output_acc: 0.6870 - val_image_quality_output_acc: 0.3165 - val_age_output_acc: 0.3654 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.3695 - val_pose_output_acc: 0.6643 - val_emotion_output_acc: 0.6815\n",
            "Epoch 37/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 488ms/step - loss: 5.9178 - gender_output_loss: 0.1738 - image_quality_output_loss: 0.5845 - age_output_loss: 1.3613 - weight_output_loss: 0.9633 - bag_output_loss: 0.7900 - footwear_output_loss: 0.5030 - pose_output_loss: 0.2409 - emotion_output_loss: 0.8098 - gender_output_acc: 0.9316 - image_quality_output_acc: 0.7366 - age_output_acc: 0.4141 - weight_output_acc: 0.6350 - bag_output_acc: 0.6551 - footwear_output_acc: 0.7753 - pose_output_acc: 0.9057 - emotion_output_acc: 0.7174 - val_loss: 11.9503 - val_gender_output_loss: 2.3880 - val_image_quality_output_loss: 1.4867 - val_age_output_loss: 1.4335 - val_weight_output_loss: 0.9975 - val_bag_output_loss: 1.0572 - val_footwear_output_loss: 1.1200 - val_pose_output_loss: 1.9310 - val_emotion_output_loss: 1.0416 - val_gender_output_acc: 0.6351 - val_image_quality_output_acc: 0.4743 - val_age_output_acc: 0.3634 - val_weight_output_acc: 0.6376 - val_bag_output_acc: 0.5645 - val_footwear_output_acc: 0.6048 - val_pose_output_acc: 0.5847 - val_emotion_output_acc: 0.6799\n",
            "Epoch 38/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 488ms/step - loss: 5.8657 - gender_output_loss: 0.1667 - image_quality_output_loss: 0.5576 - age_output_loss: 1.3615 - weight_output_loss: 0.9630 - bag_output_loss: 0.7872 - footwear_output_loss: 0.4967 - pose_output_loss: 0.2242 - emotion_output_loss: 0.8111 - gender_output_acc: 0.9344 - image_quality_output_acc: 0.7569 - age_output_acc: 0.4156 - weight_output_acc: 0.6349 - bag_output_acc: 0.6574 - footwear_output_acc: 0.7787 - pose_output_acc: 0.9117 - emotion_output_acc: 0.7163 - val_loss: 11.9078 - val_gender_output_loss: 1.7641 - val_image_quality_output_loss: 1.6121 - val_age_output_loss: 1.4364 - val_weight_output_loss: 1.0037 - val_bag_output_loss: 0.9482 - val_footwear_output_loss: 1.3104 - val_pose_output_loss: 2.2211 - val_emotion_output_loss: 1.1108 - val_gender_output_acc: 0.5050 - val_image_quality_output_acc: 0.4698 - val_age_output_acc: 0.3705 - val_weight_output_acc: 0.6421 - val_bag_output_acc: 0.5454 - val_footwear_output_acc: 0.5428 - val_pose_output_acc: 0.5978 - val_emotion_output_acc: 0.6749\n",
            "Epoch 39/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 488ms/step - loss: 5.8787 - gender_output_loss: 0.1654 - image_quality_output_loss: 0.5600 - age_output_loss: 1.3576 - weight_output_loss: 0.9628 - bag_output_loss: 0.7842 - footwear_output_loss: 0.5021 - pose_output_loss: 0.2337 - emotion_output_loss: 0.8083 - gender_output_acc: 0.9328 - image_quality_output_acc: 0.7539 - age_output_acc: 0.4181 - weight_output_acc: 0.6350 - bag_output_acc: 0.6567 - footwear_output_acc: 0.7720 - pose_output_acc: 0.9091 - emotion_output_acc: 0.7179 - val_loss: 11.8562 - val_gender_output_loss: 0.6880 - val_image_quality_output_loss: 1.7975 - val_age_output_loss: 1.4326 - val_weight_output_loss: 0.9859 - val_bag_output_loss: 0.9821 - val_footwear_output_loss: 1.5082 - val_pose_output_loss: 2.9928 - val_emotion_output_loss: 0.9608 - val_gender_output_acc: 0.7102 - val_image_quality_output_acc: 0.3810 - val_age_output_acc: 0.3664 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5333 - val_footwear_output_acc: 0.5963 - val_pose_output_acc: 0.6290 - val_emotion_output_acc: 0.6845\n",
            "Epoch 40/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 489ms/step - loss: 5.8437 - gender_output_loss: 0.1598 - image_quality_output_loss: 0.5369 - age_output_loss: 1.3589 - weight_output_loss: 0.9623 - bag_output_loss: 0.7839 - footwear_output_loss: 0.4915 - pose_output_loss: 0.2308 - emotion_output_loss: 0.8084 - gender_output_acc: 0.9370 - image_quality_output_acc: 0.7638 - age_output_acc: 0.4174 - weight_output_acc: 0.6348 - bag_output_acc: 0.6594 - footwear_output_acc: 0.7783 - pose_output_acc: 0.9109 - emotion_output_acc: 0.7165 - val_loss: 20.6710 - val_gender_output_loss: 1.0641 - val_image_quality_output_loss: 7.0523 - val_age_output_loss: 2.2489 - val_weight_output_loss: 1.2210 - val_bag_output_loss: 1.7081 - val_footwear_output_loss: 4.7595 - val_pose_output_loss: 1.1111 - val_emotion_output_loss: 0.9916 - val_gender_output_acc: 0.6321 - val_image_quality_output_acc: 0.1754 - val_age_output_acc: 0.1981 - val_weight_output_acc: 0.4209 - val_bag_output_acc: 0.5494 - val_footwear_output_acc: 0.3861 - val_pose_output_acc: 0.6799 - val_emotion_output_acc: 0.6845\n",
            "Epoch 41/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 490ms/step - loss: 5.8212 - gender_output_loss: 0.1545 - image_quality_output_loss: 0.5258 - age_output_loss: 1.3579 - weight_output_loss: 0.9613 - bag_output_loss: 0.7848 - footwear_output_loss: 0.4882 - pose_output_loss: 0.2236 - emotion_output_loss: 0.8075 - gender_output_acc: 0.9396 - image_quality_output_acc: 0.7724 - age_output_acc: 0.4161 - weight_output_acc: 0.6357 - bag_output_acc: 0.6584 - footwear_output_acc: 0.7809 - pose_output_acc: 0.9158 - emotion_output_acc: 0.7163 - val_loss: 9.2859 - val_gender_output_loss: 0.9257 - val_image_quality_output_loss: 1.4513 - val_age_output_loss: 1.4317 - val_weight_output_loss: 0.9847 - val_bag_output_loss: 0.9148 - val_footwear_output_loss: 1.1973 - val_pose_output_loss: 0.9003 - val_emotion_output_loss: 0.9591 - val_gender_output_acc: 0.7505 - val_image_quality_output_acc: 0.4924 - val_age_output_acc: 0.3826 - val_weight_output_acc: 0.6386 - val_bag_output_acc: 0.5811 - val_footwear_output_acc: 0.6169 - val_pose_output_acc: 0.7324 - val_emotion_output_acc: 0.6845\n",
            "Epoch 42/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 489ms/step - loss: 5.7866 - gender_output_loss: 0.1696 - image_quality_output_loss: 0.5000 - age_output_loss: 1.3539 - weight_output_loss: 0.9614 - bag_output_loss: 0.7796 - footwear_output_loss: 0.4823 - pose_output_loss: 0.2118 - emotion_output_loss: 0.8038 - gender_output_acc: 0.9296 - image_quality_output_acc: 0.7854 - age_output_acc: 0.4176 - weight_output_acc: 0.6357 - bag_output_acc: 0.6597 - footwear_output_acc: 0.7820 - pose_output_acc: 0.9171 - emotion_output_acc: 0.7180 - val_loss: 11.8976 - val_gender_output_loss: 1.1418 - val_image_quality_output_loss: 2.3430 - val_age_output_loss: 1.4419 - val_weight_output_loss: 0.9863 - val_bag_output_loss: 0.9347 - val_footwear_output_loss: 2.0338 - val_pose_output_loss: 1.5043 - val_emotion_output_loss: 0.9836 - val_gender_output_acc: 0.6562 - val_image_quality_output_acc: 0.3377 - val_age_output_acc: 0.3690 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5832 - val_footwear_output_acc: 0.5549 - val_pose_output_acc: 0.7056 - val_emotion_output_acc: 0.6845\n",
            "Epoch 43/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 489ms/step - loss: 5.7995 - gender_output_loss: 0.1535 - image_quality_output_loss: 0.5037 - age_output_loss: 1.3559 - weight_output_loss: 0.9622 - bag_output_loss: 0.7841 - footwear_output_loss: 0.4907 - pose_output_loss: 0.2110 - emotion_output_loss: 0.8069 - gender_output_acc: 0.9410 - image_quality_output_acc: 0.7793 - age_output_acc: 0.4168 - weight_output_acc: 0.6359 - bag_output_acc: 0.6552 - footwear_output_acc: 0.7781 - pose_output_acc: 0.9187 - emotion_output_acc: 0.7180 - val_loss: 16.6299 - val_gender_output_loss: 1.1208 - val_image_quality_output_loss: 2.2657 - val_age_output_loss: 1.4507 - val_weight_output_loss: 0.9880 - val_bag_output_loss: 1.4827 - val_footwear_output_loss: 2.4630 - val_pose_output_loss: 5.2891 - val_emotion_output_loss: 1.0348 - val_gender_output_acc: 0.6467 - val_image_quality_output_acc: 0.3503 - val_age_output_acc: 0.3674 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.2596 - val_footwear_output_acc: 0.4587 - val_pose_output_acc: 0.6300 - val_emotion_output_acc: 0.6845\n",
            "Epoch 44/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 488ms/step - loss: 5.7624 - gender_output_loss: 0.1578 - image_quality_output_loss: 0.4844 - age_output_loss: 1.3542 - weight_output_loss: 0.9609 - bag_output_loss: 0.7812 - footwear_output_loss: 0.4767 - pose_output_loss: 0.2069 - emotion_output_loss: 0.8020 - gender_output_acc: 0.9382 - image_quality_output_acc: 0.7888 - age_output_acc: 0.4167 - weight_output_acc: 0.6351 - bag_output_acc: 0.6552 - footwear_output_acc: 0.7819 - pose_output_acc: 0.9198 - emotion_output_acc: 0.7181 - val_loss: 18.8000 - val_gender_output_loss: 0.7089 - val_image_quality_output_loss: 1.1053 - val_age_output_loss: 1.4569 - val_weight_output_loss: 1.0110 - val_bag_output_loss: 0.9494 - val_footwear_output_loss: 3.0158 - val_pose_output_loss: 8.3035 - val_emotion_output_loss: 1.7069 - val_gender_output_acc: 0.7243 - val_image_quality_output_acc: 0.5575 - val_age_output_acc: 0.3679 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5459 - val_footwear_output_acc: 0.4466 - val_pose_output_acc: 0.2157 - val_emotion_output_acc: 0.6845\n",
            "Epoch 45/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 489ms/step - loss: 5.7607 - gender_output_loss: 0.1604 - image_quality_output_loss: 0.4747 - age_output_loss: 1.3540 - weight_output_loss: 0.9583 - bag_output_loss: 0.7811 - footwear_output_loss: 0.4698 - pose_output_loss: 0.2138 - emotion_output_loss: 0.8033 - gender_output_acc: 0.9389 - image_quality_output_acc: 0.7968 - age_output_acc: 0.4199 - weight_output_acc: 0.6363 - bag_output_acc: 0.6613 - footwear_output_acc: 0.7883 - pose_output_acc: 0.9192 - emotion_output_acc: 0.7181 - val_loss: 18.3974 - val_gender_output_loss: 0.7709 - val_image_quality_output_loss: 8.6454 - val_age_output_loss: 1.6808 - val_weight_output_loss: 1.1165 - val_bag_output_loss: 1.3962 - val_footwear_output_loss: 1.1101 - val_pose_output_loss: 2.0155 - val_emotion_output_loss: 1.1132 - val_gender_output_acc: 0.6971 - val_image_quality_output_acc: 0.1739 - val_age_output_acc: 0.3291 - val_weight_output_acc: 0.5373 - val_bag_output_acc: 0.5509 - val_footwear_output_acc: 0.6013 - val_pose_output_acc: 0.4995 - val_emotion_output_acc: 0.6845\n",
            "Epoch 46/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 490ms/step - loss: 5.7170 - gender_output_loss: 0.1622 - image_quality_output_loss: 0.4461 - age_output_loss: 1.3530 - weight_output_loss: 0.9589 - bag_output_loss: 0.7744 - footwear_output_loss: 0.4727 - pose_output_loss: 0.1978 - emotion_output_loss: 0.8002 - gender_output_acc: 0.9356 - image_quality_output_acc: 0.8123 - age_output_acc: 0.4175 - weight_output_acc: 0.6354 - bag_output_acc: 0.6615 - footwear_output_acc: 0.7833 - pose_output_acc: 0.9253 - emotion_output_acc: 0.7180 - val_loss: 23.2218 - val_gender_output_loss: 1.9878 - val_image_quality_output_loss: 4.5757 - val_age_output_loss: 2.0437 - val_weight_output_loss: 1.2068 - val_bag_output_loss: 1.1361 - val_footwear_output_loss: 2.0046 - val_pose_output_loss: 8.6262 - val_emotion_output_loss: 1.0856 - val_gender_output_acc: 0.6739 - val_image_quality_output_acc: 0.3191 - val_age_output_acc: 0.2107 - val_weight_output_acc: 0.5680 - val_bag_output_acc: 0.5731 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.2319 - val_emotion_output_acc: 0.5605\n",
            "Epoch 47/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 488ms/step - loss: 5.7259 - gender_output_loss: 0.1539 - image_quality_output_loss: 0.4438 - age_output_loss: 1.3554 - weight_output_loss: 0.9619 - bag_output_loss: 0.7781 - footwear_output_loss: 0.4701 - pose_output_loss: 0.2034 - emotion_output_loss: 0.8009 - gender_output_acc: 0.9381 - image_quality_output_acc: 0.8072 - age_output_acc: 0.4132 - weight_output_acc: 0.6356 - bag_output_acc: 0.6609 - footwear_output_acc: 0.7823 - pose_output_acc: 0.9222 - emotion_output_acc: 0.7176 - val_loss: 13.2599 - val_gender_output_loss: 1.4402 - val_image_quality_output_loss: 2.3446 - val_age_output_loss: 1.4893 - val_weight_output_loss: 1.0088 - val_bag_output_loss: 1.1357 - val_footwear_output_loss: 1.3118 - val_pose_output_loss: 2.8133 - val_emotion_output_loss: 1.1538 - val_gender_output_acc: 0.6925 - val_image_quality_output_acc: 0.4370 - val_age_output_acc: 0.3574 - val_weight_output_acc: 0.6290 - val_bag_output_acc: 0.5605 - val_footwear_output_acc: 0.6109 - val_pose_output_acc: 0.5146 - val_emotion_output_acc: 0.6820\n",
            "Epoch 48/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 490ms/step - loss: 5.6856 - gender_output_loss: 0.1512 - image_quality_output_loss: 0.4216 - age_output_loss: 1.3500 - weight_output_loss: 0.9595 - bag_output_loss: 0.7721 - footwear_output_loss: 0.4739 - pose_output_loss: 0.1919 - emotion_output_loss: 0.8000 - gender_output_acc: 0.9392 - image_quality_output_acc: 0.8236 - age_output_acc: 0.4201 - weight_output_acc: 0.6362 - bag_output_acc: 0.6634 - footwear_output_acc: 0.7837 - pose_output_acc: 0.9268 - emotion_output_acc: 0.7188 - val_loss: 15.6814 - val_gender_output_loss: 3.2594 - val_image_quality_output_loss: 1.6972 - val_age_output_loss: 1.4597 - val_weight_output_loss: 1.0069 - val_bag_output_loss: 1.5644 - val_footwear_output_loss: 1.3563 - val_pose_output_loss: 3.7890 - val_emotion_output_loss: 0.9795 - val_gender_output_acc: 0.4415 - val_image_quality_output_acc: 0.4834 - val_age_output_acc: 0.3664 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.3372 - val_footwear_output_acc: 0.5766 - val_pose_output_acc: 0.6568 - val_emotion_output_acc: 0.6754\n",
            "Epoch 49/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 488ms/step - loss: 5.6819 - gender_output_loss: 0.1563 - image_quality_output_loss: 0.4192 - age_output_loss: 1.3495 - weight_output_loss: 0.9604 - bag_output_loss: 0.7711 - footwear_output_loss: 0.4615 - pose_output_loss: 0.1954 - emotion_output_loss: 0.7968 - gender_output_acc: 0.9392 - image_quality_output_acc: 0.8250 - age_output_acc: 0.4182 - weight_output_acc: 0.6354 - bag_output_acc: 0.6637 - footwear_output_acc: 0.7906 - pose_output_acc: 0.9276 - emotion_output_acc: 0.7189 - val_loss: 14.4331 - val_gender_output_loss: 1.3957 - val_image_quality_output_loss: 1.8538 - val_age_output_loss: 1.6562 - val_weight_output_loss: 1.0359 - val_bag_output_loss: 1.0101 - val_footwear_output_loss: 1.9720 - val_pose_output_loss: 3.7220 - val_emotion_output_loss: 1.2121 - val_gender_output_acc: 0.6200 - val_image_quality_output_acc: 0.5010 - val_age_output_acc: 0.3412 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.4682 - val_footwear_output_acc: 0.3649 - val_pose_output_acc: 0.3800 - val_emotion_output_acc: 0.6724\n",
            "Epoch 50/50\n",
            "Learning rate:  0.1\n",
            "360/360 [==============================] - 176s 489ms/step - loss: 5.6955 - gender_output_loss: 0.1510 - image_quality_output_loss: 0.4172 - age_output_loss: 1.3486 - weight_output_loss: 0.9593 - bag_output_loss: 0.7743 - footwear_output_loss: 0.4654 - pose_output_loss: 0.2037 - emotion_output_loss: 0.7977 - gender_output_acc: 0.9408 - image_quality_output_acc: 0.8250 - age_output_acc: 0.4209 - weight_output_acc: 0.6360 - bag_output_acc: 0.6576 - footwear_output_acc: 0.7880 - pose_output_acc: 0.9239 - emotion_output_acc: 0.7177 - val_loss: 14.8320 - val_gender_output_loss: 2.1612 - val_image_quality_output_loss: 1.9229 - val_age_output_loss: 1.5276 - val_weight_output_loss: 1.0094 - val_bag_output_loss: 1.1176 - val_footwear_output_loss: 4.6304 - val_pose_output_loss: 0.9084 - val_emotion_output_loss: 0.9725 - val_gender_output_acc: 0.6522 - val_image_quality_output_acc: 0.4294 - val_age_output_acc: 0.3120 - val_weight_output_acc: 0.6280 - val_bag_output_acc: 0.5580 - val_footwear_output_acc: 0.4612 - val_pose_output_acc: 0.7223 - val_emotion_output_acc: 0.6835\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4afc230198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    }
  ]
}